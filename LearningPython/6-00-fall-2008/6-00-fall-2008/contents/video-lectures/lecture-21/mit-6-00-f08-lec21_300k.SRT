1
00:00:00 --> 00:00:00



2
00:00:00 --> 00:00:01
The following
content is provided under a

3
00:00:01 --> 00:00:03
Creative Commons license.


4
00:00:03 --> 00:00:06
Your support will help MIT
OpenCourseWare continue to

5
00:00:06 --> 00:00:10
offer high quality educational
resources for free.

6
00:00:10 --> 00:00:13
To make a donation or view
additional materials from

7
00:00:13 --> 00:00:16
hundreds of MIT courses,
visit MIT OpenCourseWare

8
00:00:16 --> 00:00:19
at ocw.mit.edu.


9
00:00:19 --> 00:00:22
PROFESSOR: So let's start.


10
00:00:22 --> 00:00:25
I have written a number
on the board here.

11
00:00:25 --> 00:00:32
Anyone want to speculate what
that number represents?

12
00:00:32 --> 00:00:34
Well, you may recall at the end
of the last lecture, we were

13
00:00:34 --> 00:00:39
simulating pi, and I started up
running it with a

14
00:00:39 --> 00:00:41
billion darts.


15
00:00:41 --> 00:00:46
And when it finally terminated,
this was the estimate of pi

16
00:00:46 --> 00:00:50
it gave me with a billion.


17
00:00:50 --> 00:00:57
Not bad, not quite perfect,
but still pretty good.

18
00:00:57 --> 00:01:00
In fact when I later ran it
with 10 billion darts, which

19
00:01:00 --> 00:01:04
took a rather long time to
run, didn't do much better.

20
00:01:04 --> 00:01:10
So it's converging very
slowly now near the end.

21
00:01:10 --> 00:01:14
When we use an algorithm like
that one to perform a Monte

22
00:01:14 --> 00:01:18
Carlo simulation, we're
trusting, as I said, that fate

23
00:01:18 --> 00:01:22
will give us an unbiased
sample, a sample that would

24
00:01:22 --> 00:01:27
be representative of
true random throws.

25
00:01:27 --> 00:01:30
And, indeed in this
case, that's a pretty

26
00:01:30 --> 00:01:31
good assumption.


27
00:01:31 --> 00:01:34
The random number generator is
not truly random, it's what's

28
00:01:34 --> 00:01:38
called pseudo-random, in that
if you start it with the same

29
00:01:38 --> 00:01:42
initial conditions, it will
give you the same results.

30
00:01:42 --> 00:01:46
But it's close enough for, at
least for government work,

31
00:01:46 --> 00:01:51
and other useful projects.


32
00:01:51 --> 00:01:55
We do have to think about
the question, how many

33
00:01:55 --> 00:01:57
samples should we run?


34
00:01:57 --> 00:02:00
Was a billion darts enough?


35
00:02:00 --> 00:02:04
Now since we sort of all
started knowing what pi was,

36
00:02:04 --> 00:02:07
we could look at it and
say, yeah, pretty good.

37
00:02:07 --> 00:02:14
But suppose we had no clue
about the actual value of pi.

38
00:02:14 --> 00:02:22
We still have to think
about the question

39
00:02:22 --> 00:02:28
of how many samples?


40
00:02:28 --> 00:02:38
And also, how accurate do we
believe our result is, given

41
00:02:38 --> 00:02:40
the number of samples?


42
00:02:40 --> 00:02:45
As you might guess, these two
questions are closely related.

43
00:02:45 --> 00:02:52
That, if we know in advance how
much accuracy we want, we can

44
00:02:52 --> 00:03:03
sometimes use that to calculate
how many samples we need.

45
00:03:03 --> 00:03:10
But there's still
always the issue.

46
00:03:10 --> 00:03:14
It's never possible to
achieve perfect accuracy

47
00:03:14 --> 00:03:16
through sampling.


48
00:03:16 --> 00:03:20
Unless you sample the
entire population.

49
00:03:20 --> 00:03:25
No matter how many samples you
take, you can never be sure

50
00:03:25 --> 00:03:29
that the sample set is typical
until you've checked

51
00:03:29 --> 00:03:32
every last element.


52
00:03:32 --> 00:03:38
So if I went around MIT and
sampled 100 students to try

53
00:03:38 --> 00:03:43
and, for example, guess the
fraction of students at MIT

54
00:03:43 --> 00:03:46
who are of Chinese descent.


55
00:03:46 --> 00:03:52
Maybe 100 students would be
enough, but maybe I would get

56
00:03:52 --> 00:03:55
unlucky and draw the wrong 100.


57
00:03:55 --> 00:03:59
In the sense of, by accident,
100 Chinese descent, or 100

58
00:03:59 --> 00:04:04
non-Chinese descent, which
would give me the wrong answer.

59
00:04:04 --> 00:04:08
And there would be no way I
could be sure that I had not

60
00:04:08 --> 00:04:18
drawn a biased sample, unless
I really did have the whole

61
00:04:18 --> 00:04:22
population to look at.


62
00:04:22 --> 00:04:28
So we can never know that
our estimate is correct.

63
00:04:28 --> 00:04:32
Now maybe I took a billion
darts, and for some reason

64
00:04:32 --> 00:04:35
got really unlucky and they
all ended up inside or

65
00:04:35 --> 00:04:38
outside the circle.


66
00:04:38 --> 00:04:42
But what we can know, is how
likely it is that our answer is

67
00:04:42 --> 00:04:46
correct, given the assumptions.


68
00:04:46 --> 00:04:48
And that's the topic we'll
spend the next few lectures on,

69
00:04:48 --> 00:04:50
at least one of the topics.


70
00:04:50 --> 00:04:54
It's saying, how can we
know how likely it is

71
00:04:54 --> 00:04:56
that our answer is good.


72
00:04:56 --> 00:05:01
But it's always given some set
of assumptions, and we have

73
00:05:01 --> 00:05:04
to worry a lot about
those assumptions.

74
00:05:04 --> 00:05:10
Now in the case of our pi
example, our assumption was

75
00:05:10 --> 00:05:14
that the random number
generator was indeed giving

76
00:05:14 --> 00:05:18
us random numbers in
the interval 0 to 1.

77
00:05:18 --> 00:05:23
So that was our
underlying assumption.

78
00:05:23 --> 00:05:28
Then using that, we looked at a
plot, and we saw that after

79
00:05:28 --> 00:05:33
time the answer wasn't
changing very much.

80
00:05:33 --> 00:05:36
And we use that to say, OK,
it looks like we're actually

81
00:05:36 --> 00:05:40
converging on an answer.


82
00:05:40 --> 00:05:45
And then I ran it again, with
another trial, and it converged

83
00:05:45 --> 00:05:49
again at the same place.


84
00:05:49 --> 00:05:53
And the fact that that happened
several times led me to at

85
00:05:53 --> 00:05:56
least have some reason
to believe that I was

86
00:05:56 --> 00:06:04
actually finding a good
approximation of pi.

87
00:06:04 --> 00:06:07
That's a good thing to do.


88
00:06:07 --> 00:06:09
It's a necessary thing to do.


89
00:06:09 --> 00:06:11
But it is not sufficient.


90
00:06:11 --> 00:06:16
Because errors can creep
into many places.

91
00:06:16 --> 00:06:20
So that kind of technique, and
in fact, almost all statistical

92
00:06:20 --> 00:06:26
techniques, are good at
establishing, in some sense,

93
00:06:26 --> 00:06:30
the reproduce-ability of the
result, and that it is

94
00:06:30 --> 00:06:34
statistically valid, and
that there's no error, for

95
00:06:34 --> 00:06:40
example, in the way I'm
generating the numbers.

96
00:06:40 --> 00:06:43
Or I didn't get very unlucky.


97
00:06:43 --> 00:06:48
However, they're other places
other than bad luck where

98
00:06:48 --> 00:06:51
errors can creep in.


99
00:06:51 --> 00:06:53
So let's look at
an example here.

100
00:06:53 --> 00:07:00
I've taken the algorithm we
looked at last time for finding

101
00:07:00 --> 00:07:08
pi, and I've made a change.


102
00:07:08 --> 00:07:13
You'll remember that we were
before using 4 as our

103
00:07:13 --> 00:07:16
multiplier, and here what
I've done is, just gone

104
00:07:16 --> 00:07:20
in and replaced 4 by 2.


105
00:07:20 --> 00:07:25
Assuming that I made
a programming error.

106
00:07:25 --> 00:07:35
Now let's see what
happens when we run it.

107
00:07:35 --> 00:07:42
Well, a bad thing has happened.


108
00:07:42 --> 00:07:50
Sure enough, we ran it and it
converged, started to converge,

109
00:07:50 --> 00:07:53
and if I ran 100 trials
each one would converge at

110
00:07:53 --> 00:07:56
roughly the same place.


111
00:07:56 --> 00:08:00
Any statistical test I would
do, would say that my

112
00:08:00 --> 00:08:03
statistics are sound, I've
chosen enough samples, and for

113
00:08:03 --> 00:08:05
some accuracy, it's converting.


114
00:08:05 --> 00:08:10
Everything is perfect,
except for what?

115
00:08:10 --> 00:08:13
It's the wrong answer.


116
00:08:13 --> 00:08:18
The moral here, is that
just because an answer is

117
00:08:18 --> 00:08:45
statistically valid, does not
mean it's the right answer.

118
00:08:45 --> 00:08:49
And that's really important to
understand, because you see

119
00:08:49 --> 00:08:53
this, and we'll see more
examples later, not today, but

120
00:08:53 --> 00:08:57
after Thanksgiving, comes up
all the time in the newspapers,

121
00:08:57 --> 00:09:01
in scientific articles, where
people do a million tests, do

122
00:09:01 --> 00:09:05
all the statistics right, say
here's the answer, and it turns

123
00:09:05 --> 00:09:08
out to be completely wrong.


124
00:09:08 --> 00:09:12
And that's because it was some
underlying assumption that went

125
00:09:12 --> 00:09:17
into the decision,
that was not true.

126
00:09:17 --> 00:09:20
So here, the assumption is,
that I've done my algebra

127
00:09:20 --> 00:09:27
right for computing pi based
upon where the darts land.

128
00:09:27 --> 00:09:32
And it turns out, if I put 2
here, my algebra is wrong.

129
00:09:32 --> 00:09:35
Now how could I discover this?


130
00:09:35 --> 00:09:38
Since I've already told
you no statistical test

131
00:09:38 --> 00:09:40
is going to help me.


132
00:09:40 --> 00:09:42
What's the obvious thing
I should be doing when

133
00:09:42 --> 00:09:44
I get this answer?


134
00:09:44 --> 00:09:45
Somebody?


135
00:09:45 --> 00:09:50
Yeah?


136
00:09:50 --> 00:09:50
STUDENT: [INAUDIBLE]


137
00:09:50 --> 00:09:54
PROFESSOR: Exactly.


138
00:09:54 --> 00:09:57
Checking against reality.


139
00:09:57 --> 00:10:00
I started with the notion
that pi had some relation

140
00:10:00 --> 00:10:03
to the area of a circle.


141
00:10:03 --> 00:10:08
So I could use this value
of pi, draw a circle

142
00:10:08 --> 00:10:11
with a radius.


143
00:10:11 --> 00:10:13
Do my best to measure the area.


144
00:10:13 --> 00:10:17
I wouldn't need to get a very
good, accurate measurement,

145
00:10:17 --> 00:10:20
and I would say, whoa,
this isn't even close.

146
00:10:20 --> 00:10:25
And that would tell
me I have a problem.

147
00:10:25 --> 00:10:37
So the moral here is, to
check results against

148
00:10:37 --> 00:10:50
physical reality.


149
00:10:50 --> 00:10:53
So for example, the current
problem set, you're doing a

150
00:10:53 --> 00:10:56
simulation about what
happens to viruses when

151
00:10:56 --> 00:11:00
drugs are applied.


152
00:11:00 --> 00:11:03
If you were doing this for a
pharmaceutical company, in

153
00:11:03 --> 00:11:06
addition to the simulation,
you'd want to run some

154
00:11:06 --> 00:11:08
real experiments.


155
00:11:08 --> 00:11:17
And make sure that
things matched.

156
00:11:17 --> 00:11:30
OK, what this suggests, is that
we often use simulation, and

157
00:11:30 --> 00:11:36
other computational techniques,
to try and model the real

158
00:11:36 --> 00:11:42
world, or the physical world,
in which we all live.

159
00:11:42 --> 00:11:46
And we can use data to do that.


160
00:11:46 --> 00:11:51
I now want to go through
another set of examples, and

161
00:11:51 --> 00:11:55
we're going to look at the
interplay of three things: what

162
00:11:55 --> 00:12:02
happens when you have data, say
from measurements, and

163
00:12:02 --> 00:12:15
models that at least claim
to explain the data.

164
00:12:15 --> 00:12:26
And then, consequences that
follow from the models.

165
00:12:26 --> 00:12:30
This is often the way science
works, its the way engineering

166
00:12:30 --> 00:12:35
works, we have some
measurements, we have a theory

167
00:12:35 --> 00:12:40
that explains the measurements,
and then we write software to

168
00:12:40 --> 00:12:43
explore the consequences
of that theory.

169
00:12:43 --> 00:12:48
Including, is it plausible
that it's really true?

170
00:12:48 --> 00:12:53
So I want to start, as an
example, with a classic

171
00:12:53 --> 00:12:57
chosen from 8.01.


172
00:12:57 --> 00:13:01
So I presume, everyone
here has taken 8.01?

173
00:13:01 --> 00:13:02
Or in 8.01?


174
00:13:02 --> 00:13:08
Anyone here who's not had
an experience with 801?

175
00:13:08 --> 00:13:11
All right, well.


176
00:13:11 --> 00:13:13
I hope you know about springs,
because we're going to

177
00:13:13 --> 00:13:15
talk about springs.


178
00:13:15 --> 00:13:18
So if you think about it, I'm
now just talking not about

179
00:13:18 --> 00:13:21
springs that have water in
them, but springs that you

180
00:13:21 --> 00:13:25
compress, you know, and
expand, and things like that.

181
00:13:25 --> 00:13:35
And there's typically something
called the spring constant that

182
00:13:35 --> 00:13:43
tells us how stiff the spring
is, how much energy it takes

183
00:13:43 --> 00:13:45
to compress this spring.


184
00:13:45 --> 00:13:49
Or equivalently, how much pop
the spring has when you're

185
00:13:49 --> 00:13:53
no longer holding it down.


186
00:13:53 --> 00:13:56
Some springs are easy to
stretch, they have a

187
00:13:56 --> 00:13:58
small spring constant.


188
00:13:58 --> 00:14:01
Some strings, for example,
the ones that hold up an

189
00:14:01 --> 00:14:04
automobile, suspension,
are much harder to

190
00:14:04 --> 00:14:08
stretch and compress.


191
00:14:08 --> 00:14:20
There's a theory about
them called Hooke's Law.

192
00:14:20 --> 00:14:28
And it's quite simple.


193
00:14:28 --> 00:14:32
Force, the amount of force
exerted by a spring, is equal

194
00:14:32 --> 00:14:38
to minus some constant times
the distance you have

195
00:14:38 --> 00:14:44
compressed the spring.


196
00:14:44 --> 00:14:47
It's minus, because the force
is exerted in an opposite

197
00:14:47 --> 00:14:50
direction, trying to spring up.


198
00:14:50 --> 00:14:55
So for example, we could
look at it this way.

199
00:14:55 --> 00:15:01
We've got a spring,
excuse my art here.

200
00:15:01 --> 00:15:05
And we put some weight on the
spring, which has therefore

201
00:15:05 --> 00:15:08
compressed it a little bit.


202
00:15:08 --> 00:15:13
And the spring is exerting
some upward force.

203
00:15:13 --> 00:15:17
And the amount of force it's
exerting is proportional

204
00:15:17 --> 00:15:26
to the distance x.


205
00:15:26 --> 00:15:34
So, if we believe Hooke's Law,
and I give you a spring, how

206
00:15:34 --> 00:15:38
can we find out what
this constant is?

207
00:15:38 --> 00:15:45
Well, we can do it by putting a
weight on top of the spring.

208
00:15:45 --> 00:15:50
It will compress the spring a
certain amount, and then the

209
00:15:50 --> 00:15:53
spring will stop moving.


210
00:15:53 --> 00:15:56
Now gravity would normally have
had this weight go all the way

211
00:15:56 --> 00:16:00
down to the bottom, if
there was no spring.

212
00:16:00 --> 00:16:03
So clearly the spring is
exerting some force in the

213
00:16:03 --> 00:16:08
upward direction, to keep
that mass from going down

214
00:16:08 --> 00:16:14
to the table, right?


215
00:16:14 --> 00:16:17
So we know what that
force is there.

216
00:16:17 --> 00:16:23
If we compress the spring to a
bunch of different distances,

217
00:16:23 --> 00:16:31
by putting, say, different size
weights on it, we can then

218
00:16:31 --> 00:16:36
solve for the spring constant,
just the way, before,

219
00:16:36 --> 00:16:39
we solved for pi.


220
00:16:39 --> 00:16:47
So it just so happens, not
quite by accident, that I've

221
00:16:47 --> 00:16:50
got some data from a spring.


222
00:16:50 --> 00:16:52
So let's look at it.


223
00:16:52 --> 00:16:57
So here's some data taken
from measuring a spring.

224
00:16:57 --> 00:17:00
This is distance and force,
force computed from the

225
00:17:00 --> 00:17:02
mass, basically, right?


226
00:17:02 --> 00:17:07
Because we know that these
have to be in balance.

227
00:17:07 --> 00:17:10
And I'm not going to ask you
to in your head estimate the

228
00:17:10 --> 00:17:16
constant from these, but what
you'll see is, the format is,

229
00:17:16 --> 00:17:22
there's a distance, and then
a colon, and then the force.

230
00:17:22 --> 00:17:22
Yeah?


231
00:17:22 --> 00:17:29
STUDENT: [INAUDIBLE]


232
00:17:29 --> 00:17:37
PROFESSOR: Ok, right,
yes, thank you.

233
00:17:37 --> 00:17:41
All right, want to repeat that
more loudly for everyone?

234
00:17:41 --> 00:17:42
STUDENT: [INAUDIBLE]


235
00:17:42 --> 00:17:48
PROFESSOR: Right, right,
because the x in the equation

236
00:17:48 --> 00:17:53
-- right, here we're
getting an equilibrium.

237
00:17:53 --> 00:17:56
OK, so let's look at what
happens when we try

238
00:17:56 --> 00:17:59
and examine this.


239
00:17:59 --> 00:18:05
We'll look at spring dot pi.


240
00:18:05 --> 00:18:07
So it's pretty simple.


241
00:18:07 --> 00:18:10
First thing is, I've got
a function that reads in

242
00:18:10 --> 00:18:12
the data and parses it.


243
00:18:12 --> 00:18:15
You've all done more
complicated parsing of

244
00:18:15 --> 00:18:16
data files than this.


245
00:18:16 --> 00:18:19
So I won't belabor the details.


246
00:18:19 --> 00:18:22
I called it get data rather
than get spring data, because

247
00:18:22 --> 00:18:24
I'm going to use the
same thing for a lot of

248
00:18:24 --> 00:18:26
other kinds of data.


249
00:18:26 --> 00:18:29
And the only thing I want you
to notice, is that it's

250
00:18:29 --> 00:18:36
returning a pair of arrays.


251
00:18:36 --> 00:18:38
OK, not lists.


252
00:18:38 --> 00:18:41
The usual thing is, I'm
building them up using lists,

253
00:18:41 --> 00:18:44
because lists have append and
arrays don't, and then I'm

254
00:18:44 --> 00:18:48
converting them to arrays so I
can do matrix kinds of

255
00:18:48 --> 00:18:50
operations on them.


256
00:18:50 --> 00:18:54
So I'll get the distances
and the forces.

257
00:18:54 --> 00:18:56
And then I'm just going to
plot them, and we'll see

258
00:18:56 --> 00:18:58
what they look like.


259
00:18:58 --> 00:19:10
So let's do that.


260
00:19:10 --> 00:19:14
There they are.


261
00:19:14 --> 00:19:19
Now, if you believe Hooke's
Law, you could look at this

262
00:19:19 --> 00:19:25
data, and maybe you
wouldn't like it.

263
00:19:25 --> 00:19:28
Because Hooke's Law implies
that, in fact, these

264
00:19:28 --> 00:19:33
points should lie in a
straight line, right?

265
00:19:33 --> 00:19:44
If I just plug in values here,
what am I going to get?

266
00:19:44 --> 00:19:46
A straight line, right?


267
00:19:46 --> 00:19:49
I'm just multiplying k times x.


268
00:19:49 --> 00:19:51
But I don't have a straight
line, I have a little scatter

269
00:19:51 --> 00:19:54
of points, kind of it
looks like a straight

270
00:19:54 --> 00:19:57
line, but it's not.


271
00:19:57 --> 00:19:59
And why do you
think that's true?

272
00:19:59 --> 00:20:03
What's going on here?


273
00:20:03 --> 00:20:12
What could cause this
line not to be straight?

274
00:20:12 --> 00:20:17
Have any you ever done
a physics experiment?

275
00:20:17 --> 00:20:21
And when you did it, did your
results actually match the

276
00:20:21 --> 00:20:27
theory that your high school
teacher, say, explained to you.

277
00:20:27 --> 00:20:30
No, and why not.


278
00:20:30 --> 00:20:34
Yeah, you have various
kinds of experimental or

279
00:20:34 --> 00:20:39
measurement error, right?


280
00:20:39 --> 00:20:44
Because, when you're doing
these experiments, at least I'm

281
00:20:44 --> 00:20:47
not perfect, and I suspect at
least most of you are not

282
00:20:47 --> 00:20:50
perfect, you get mistakes.


283
00:20:50 --> 00:20:54
A little bit of error
creeps in inevitably.

284
00:20:54 --> 00:20:57
And so, when we acquired this
data, sure enough there

285
00:20:57 --> 00:21:00
was measurement error.


286
00:21:00 --> 00:21:04
And so the points are
scattered around.

287
00:21:04 --> 00:21:06
This is something
to be expected.

288
00:21:06 --> 00:21:13
Real data almost never matches
the theory precisely.

289
00:21:13 --> 00:21:16
Because there usually is some
sort of experimental error

290
00:21:16 --> 00:21:24
that creeps into things.


291
00:21:24 --> 00:21:28
So what should we
do about that?

292
00:21:28 --> 00:21:32
Well, what usually people do,
when they think about this, is

293
00:21:32 --> 00:21:35
they would look at this data
and say, well, let me

294
00:21:35 --> 00:21:37
fit a line to this.


295
00:21:37 --> 00:21:43
Somehow, say, what would
be the line that best

296
00:21:43 --> 00:21:47
approximates these points?


297
00:21:47 --> 00:21:51
And then the slope of
that line would give me

298
00:21:51 --> 00:21:57
the spring constant.


299
00:21:57 --> 00:22:03
So that raises the next
question, what do I mean

300
00:22:03 --> 00:22:09
by finding a line that
best fits these points?

301
00:22:09 --> 00:22:26
How do we, fit, in this
case, a line, to the data?

302
00:22:26 --> 00:22:29
First of all, I should ask
the question, why did I

303
00:22:29 --> 00:22:31
say let's fit a line?


304
00:22:31 --> 00:22:33
Maybe I should have said,
let's fit a parabola,

305
00:22:33 --> 00:22:38
or let's fit a circle?


306
00:22:38 --> 00:22:45
Why should I had said
let's fit a line.

307
00:22:45 --> 00:22:45
Yeah?


308
00:22:45 --> 00:22:49
STUDENT: [INAUDIBLE]


309
00:22:49 --> 00:22:51
PROFESSOR: Well, how do
I know that the plot

310
00:22:51 --> 00:22:57
is a linear function?


311
00:22:57 --> 00:23:01
Pardon?


312
00:23:01 --> 00:23:04
Well, so, two things.


313
00:23:04 --> 00:23:08
One is, I had a theory.


314
00:23:08 --> 00:23:12
You know, I had up there a
model, and my model

315
00:23:12 --> 00:23:17
suggested that I expected
it to be linear.

316
00:23:17 --> 00:23:21
And so if I'm testing my model,
I should and fit a line,

317
00:23:21 --> 00:23:23
my theory, if you will.


318
00:23:23 --> 00:23:26
But also when I look at it, it
looks kind of like a line.

319
00:23:26 --> 00:23:30
So you know, if I looked at it,
and it didn't look like a line,

320
00:23:30 --> 00:23:34
I might have said, well, my
model must be badly broken.

321
00:23:34 --> 00:23:38
So let's try and see
if we can fit it.

322
00:23:38 --> 00:23:43
Whenever we try and fit
something, we need some sort of

323
00:23:43 --> 00:23:56
an objective function that
captures the goodness of a fit.

324
00:23:56 --> 00:23:59
I'm trying to find, this is
an optimization problem of

325
00:23:59 --> 00:24:01
the sort that we've
looked at before.

326
00:24:01 --> 00:24:06
I'm trying to find a line
that optimizes some

327
00:24:06 --> 00:24:10
objective function.


328
00:24:10 --> 00:24:15
So a very simple objective
function here, is called

329
00:24:15 --> 00:24:24
the least squares fit.


330
00:24:24 --> 00:24:37
I want to find the line
that minimizes the sum of

331
00:24:37 --> 00:24:47
observation sub i, the i'th
data point I have, minus what

332
00:24:47 --> 00:24:54
the line, the model, predicts
that point should have been,

333
00:24:54 --> 00:24:59
and then I'll square it.


334
00:24:59 --> 00:25:03
So I want to minimize
this value.

335
00:25:03 --> 00:25:07
I want to find the line
that gives me the

336
00:25:07 --> 00:25:10
smallest value for this.


337
00:25:10 --> 00:25:12
Why do you think I'm
squaring the difference?

338
00:25:12 --> 00:25:17
What would happen if I didn't
square the difference?

339
00:25:17 --> 00:25:18
Yeah?


340
00:25:18 --> 00:25:24
Positive and negative errors
might cancel each other out.

341
00:25:24 --> 00:25:28
And in judging the quality of
the fit, I don't really care

342
00:25:28 --> 00:25:31
deeply -- you're going to get
very fat the way you're

343
00:25:31 --> 00:25:36
collecting candy here -- I
don't care deeply whether the

344
00:25:36 --> 00:25:39
error is, which side, it
is, just that it's wrong.

345
00:25:39 --> 00:25:43
And so by squaring it, it's
kind of like taking the

346
00:25:43 --> 00:25:47
absolute value of the
error, among other things.

347
00:25:47 --> 00:25:54
All right, so if we look
at our example here,

348
00:25:54 --> 00:26:02
what would this be?


349
00:26:02 --> 00:26:07
I want to minimize, want to
find a line that minimizes it.

350
00:26:07 --> 00:26:10
So how do I do that?


351
00:26:10 --> 00:26:13
I could easily do it
using successive

352
00:26:13 --> 00:26:17
approximation, right?


353
00:26:17 --> 00:26:20
I could choose a line,
basically what I am, is I'm

354
00:26:20 --> 00:26:23
choosing a slope, here, right?


355
00:26:23 --> 00:26:27
And, I could, just like Newton
Raphson, do successive

356
00:26:27 --> 00:26:34
approximation for awhile,
and get the best fit.

357
00:26:34 --> 00:26:37
That's one way to do
the optimization.

358
00:26:37 --> 00:26:42
It turns out that for this
particular optimization there's

359
00:26:42 --> 00:26:44
something more efficient.


360
00:26:44 --> 00:26:48
You can actually, there is a
closed form way of attacking

361
00:26:48 --> 00:26:52
this, and I could explain that,
but in fact, I'll explain

362
00:26:52 --> 00:26:55
something even better.


363
00:26:55 --> 00:27:04
It's built into Pylab.


364
00:27:04 --> 00:27:14
So Pylab has a function
built-in called polyfit.

365
00:27:14 --> 00:27:19
Which, given a set of points,
finds the polynomial that gives

366
00:27:19 --> 00:27:28
you the best least squares
approximation to those points.

367
00:27:28 --> 00:27:33
It's called polynomial because
it isn't necessarily going to

368
00:27:33 --> 00:27:36
be first order, that
is to say, a line.

369
00:27:36 --> 00:27:42
It can find polynomials
of arbitrary degree.

370
00:27:42 --> 00:27:48
So let's look at the example
here, we'll see how it works.

371
00:27:48 --> 00:27:59
So let me uncomment it.


372
00:27:59 --> 00:28:07
So I'm going to get k and b
equals Pylab dot polyfit here.

373
00:28:07 --> 00:28:14
What it's going to do is,
think about a polynomial.

374
00:28:14 --> 00:28:18
I give you a polynomial of
degree one, you have all

375
00:28:18 --> 00:28:25
learned that it's a x plus
b, b is the constant, and

376
00:28:25 --> 00:28:29
x is the single variable.


377
00:28:29 --> 00:28:33
And so I multiply a by x and
I add b to it, and as I

378
00:28:33 --> 00:28:36
vary x I get new values.


379
00:28:36 --> 00:28:47
And so polyfit, in this case,
will take the set of points

380
00:28:47 --> 00:28:52
defined by these two arrays
and return me a value

381
00:28:52 --> 00:28:57
for a and a value for b.


382
00:28:57 --> 00:29:05
Now here I've assigned a to k,
but don't worry about that.

383
00:29:05 --> 00:29:11
And then, I'm gonna now
generate the predictions that

384
00:29:11 --> 00:29:19
I would get from this k
and b, and plot those.

385
00:29:19 --> 00:29:32
So let's look at it.


386
00:29:32 --> 00:29:39
So here it said the k is
31.475, etc., and it's plotted

387
00:29:39 --> 00:29:43
the line that it's found.


388
00:29:43 --> 00:29:45
Or I've plotted the line.


389
00:29:45 --> 00:29:48
You'll note, a lot of the
points don't lie on the line,

390
00:29:48 --> 00:29:53
in fact, most of the points
don't lie on the line.

391
00:29:53 --> 00:29:56
But it's asserting that
this is the best it

392
00:29:56 --> 00:29:58
can do with the line.


393
00:29:58 --> 00:30:02
And there's some points, for
example, up here, that are

394
00:30:02 --> 00:30:07
kind of outliers, that are
pretty far from the line.

395
00:30:07 --> 00:30:11
But it has minimized the
error, if you will, for

396
00:30:11 --> 00:30:15
all of the points it has.


397
00:30:15 --> 00:30:18
That's quite different from,
say, finding the line that

398
00:30:18 --> 00:30:22
touches the most points, right?


399
00:30:22 --> 00:30:29
It's minimizing the
sum of the errors.

400
00:30:29 --> 00:30:32
Now, given that I was just
looking for a constant to

401
00:30:32 --> 00:30:40
start with, why did I bother
even plotting the data?

402
00:30:40 --> 00:30:44
I happen to have known before I
did this that polyfit existed,

403
00:30:44 --> 00:30:49
and what I was really
looking for was this line.

404
00:30:49 --> 00:30:51
So maybe I should have just
done the polyfit and said

405
00:30:51 --> 00:30:55
here's k and I'm done.


406
00:30:55 --> 00:31:01
Would that have
been a good idea?

407
00:31:01 --> 00:31:01
Yeah?


408
00:31:01 --> 00:31:07
STUDENT: You can't know without
seeing the actual data how well

409
00:31:07 --> 00:31:10
it's actually fitting it.


410
00:31:10 --> 00:31:11
PROFESSOR: Right.


411
00:31:11 --> 00:31:12
Exactly right.


412
00:31:12 --> 00:31:15
That says, well how would I
know that it was fitting it

413
00:31:15 --> 00:31:19
badly or well, and in fact, how
would I know that my notion of

414
00:31:19 --> 00:31:23
the model is sound, or that
my experiment isn't

415
00:31:23 --> 00:31:25
completely broken?


416
00:31:25 --> 00:31:31
So always, I think, always
look at the real data.

417
00:31:31 --> 00:31:34
Don't just, I've seen too many
papers where people show me the

418
00:31:34 --> 00:31:38
curve that fits the data, and
don't show me the data, and it

419
00:31:38 --> 00:31:40
always makes me very nervous.


420
00:31:40 --> 00:31:44
So always look at the data,
as well as however you're

421
00:31:44 --> 00:31:46
choosing to fit it.


422
00:31:46 --> 00:31:53
As an example of that, let's
look at another set of inputs.

423
00:31:53 --> 00:32:05
This is not a spring.


424
00:32:05 --> 00:32:09
It's the same get data function
as before, ignore that

425
00:32:09 --> 00:32:13
thing at the top.


426
00:32:13 --> 00:32:26
I'm going to analyze it
and we'll look at it.

427
00:32:26 --> 00:32:33
So here I'm plotting the speed
of something over time.

428
00:32:33 --> 00:32:39
So I plotted it, and I've done
a least squares fit using

429
00:32:39 --> 00:32:44
polyfit just as before to get a
line, and I put the line vs.

430
00:32:44 --> 00:32:51
the data, and here I'm
a little suspicious.

431
00:32:51 --> 00:32:56
Right, I fit a line, but when I
look at it, I don't think it's

432
00:32:56 --> 00:32:59
a real good fit for the data.


433
00:32:59 --> 00:33:14
Somehow modeling this data as
a line is probably not right.

434
00:33:14 --> 00:33:17
A linear model is not
good for this data.

435
00:33:17 --> 00:33:20
This data is derived
from something, a

436
00:33:20 --> 00:33:23
more complex process.


437
00:33:23 --> 00:33:27
So take a look at it, and tell
me what order were calling of

438
00:33:27 --> 00:33:29
polynomial do you think
might fit this data?

439
00:33:29 --> 00:33:34
What shape does this
look like to you?

440
00:33:34 --> 00:33:35
Pardon?


441
00:33:35 --> 00:33:36
STUDENT: Quadratic.


442
00:33:36 --> 00:33:40
PROFESSOR: Quadratic, because
the shape is a what?

443
00:33:40 --> 00:33:41
It's a parabola.


444
00:33:41 --> 00:33:43
Well, I don't know if I
dare try this one all

445
00:33:43 --> 00:33:45
the way to the back.


446
00:33:45 --> 00:33:50
Ooh, at least I
didn't hurt anybody.

447
00:33:50 --> 00:33:53
All right, fortunately it's
just as easy to fit a

448
00:33:53 --> 00:33:59
ravel parabola as a line.


449
00:33:59 --> 00:34:06
So let's look down here.


450
00:34:06 --> 00:34:11
I've done the same thing, but
instead of passing it one, as I

451
00:34:11 --> 00:34:15
did up here as the argument,
I'm passing it two.

452
00:34:15 --> 00:34:18
Saying, instead of fitting a
polynomial of degree one, fit

453
00:34:18 --> 00:34:21
a polynomial of degree two.


454
00:34:21 --> 00:34:32
And now let's see
what it looks like.

455
00:34:32 --> 00:34:44
Well, my eyes tell me this is a
much better fit than the line.

456
00:34:44 --> 00:34:49
So again, that's why I wanted
to see the scatter plot, so

457
00:34:49 --> 00:34:53
that I could at least look at
it with my eyes, and say, yeah,

458
00:34:53 --> 00:34:58
this looks like a better fit.


459
00:34:58 --> 00:35:07
All right, any question
about what's going on here?

460
00:35:07 --> 00:35:12
What we've been looking
at is something called

461
00:35:12 --> 00:35:23
linear regression.


462
00:35:23 --> 00:35:30
It's called linear because the
relationship of the dependent

463
00:35:30 --> 00:35:39
variable y to the independent
variables is assumed to be a

464
00:35:39 --> 00:35:43
linear function of
the parameters.

465
00:35:43 --> 00:35:46
It's not because it has
to be a linear function

466
00:35:46 --> 00:35:50
of the value of x, OK?


467
00:35:50 --> 00:35:53
Because as you can see, we're
not getting a line, we're

468
00:35:53 --> 00:35:56
getting a parabola.


469
00:35:56 --> 00:36:00
Don't worry about the details,
the point I want to make is,

470
00:36:00 --> 00:36:03
people sometimes see the word
linear regression and think it

471
00:36:03 --> 00:36:06
can only be used to find lines.


472
00:36:06 --> 00:36:11
It's not so.


473
00:36:11 --> 00:36:16
So when, for example, we did
the quadratic, what we had is

474
00:36:16 --> 00:36:26
y equals a x squared
plus b x plus c.

475
00:36:26 --> 00:36:30
The graph vs. x will not be
a straight line, right,

476
00:36:30 --> 00:36:34
because I'm squaring x.


477
00:36:34 --> 00:36:43
But it is, just about, in this
case, the single variable x.

478
00:36:43 --> 00:36:49
Now, when I looked at this, I
said, all right, it's clear

479
00:36:49 --> 00:36:55
that the yellow curve is a
better fit than the red.

480
00:36:55 --> 00:36:59
It's a red line.


481
00:36:59 --> 00:37:03
But that was a pretty
informal statement.

482
00:37:03 --> 00:37:11
I can actually look at
this much more formally.

483
00:37:11 --> 00:37:14
And we're going to look
at something that's the

484
00:37:14 --> 00:37:18
statisticians call r squared.


485
00:37:18 --> 00:37:26
Which in the case of a linear
regression is the coefficient

486
00:37:26 --> 00:37:34
of determination.


487
00:37:34 --> 00:37:38
Now, this is a big fancy
word for something that's

488
00:37:38 --> 00:37:41
actually pretty simple.


489
00:37:41 --> 00:37:44
So what r squared its going
to be, and this is on your

490
00:37:44 --> 00:37:56
handout, is 1 minus
e e over d v.

491
00:37:56 --> 00:38:02
So e e is going to be the
errors in the estimation.

492
00:38:02 --> 00:38:06
So I've got some estimated
values, some predicted values,

493
00:38:06 --> 00:38:11
if you will, given to me by
the model, either the line or

494
00:38:11 --> 00:38:14
the parabola in this case.


495
00:38:14 --> 00:38:18
And I've got some real values,
corresponding to each of those

496
00:38:18 --> 00:38:25
points, and I can look at the
difference between the 2 And

497
00:38:25 --> 00:38:29
that will tell me how much
difference there is between the

498
00:38:29 --> 00:38:36
estimated data and the, well,
between the predicted data and

499
00:38:36 --> 00:38:42
the measured data,
in this case.

500
00:38:42 --> 00:38:48
And then I want to divide
that by the variance

501
00:38:48 --> 00:38:51
in the measured data.


502
00:38:51 --> 00:38:59
The data variance.


503
00:38:59 --> 00:39:03
How broadly scattered the
measured points are.

504
00:39:03 --> 00:39:08
And I'll do that by comparing
the mean of the measured

505
00:39:08 --> 00:39:13
data, to the measured data.


506
00:39:13 --> 00:39:15
So I get the average value
of the measured data, and I

507
00:39:15 --> 00:39:21
look at how different the
points I measure are.

508
00:39:21 --> 00:39:26
So I just want to give to
you, informally, because I

509
00:39:26 --> 00:39:28
really don't care if you
understand all the math.

510
00:39:28 --> 00:39:32
What I do want you to
understand, when someone tells

511
00:39:32 --> 00:39:37
you, here's the r squared
value, is, informally what

512
00:39:37 --> 00:39:39
it really is saying.


513
00:39:39 --> 00:39:47
It's attempting to capture the
proportion of the response

514
00:39:47 --> 00:39:51
variation explained by the
variables in the model.

515
00:39:51 --> 00:39:56
In this case, x.


516
00:39:56 --> 00:40:04
So you'll have some amount of
variation that is explained by

517
00:40:04 --> 00:40:08
changing the values
of the variables.

518
00:40:08 --> 00:40:11
So if, actually, I'm going to
give an example and then come

519
00:40:11 --> 00:40:12
back to it more informally.


520
00:40:12 --> 00:40:21
So if, for example, r squared
were to equal 0.9, that would

521
00:40:21 --> 00:40:26
mean that approximately 90
percent of the variation

522
00:40:26 --> 00:40:34
in the variables can be
explained by the model.

523
00:40:34 --> 00:40:37
OK, so we have some amount of
variation in the measured data,

524
00:40:37 --> 00:40:42
and if r squared is 0.9, it
says that 90 percent can be

525
00:40:42 --> 00:40:49
explained by the models, and
the other 10 percent cannot.

526
00:40:49 --> 00:40:54
Now, that other 10 percent
could be experimental error, or

527
00:40:54 --> 00:40:58
it could be that, in fact, you
need more variables

528
00:40:58 --> 00:41:00
in the model.


529
00:41:00 --> 00:41:05
That there are what are
called lurking variables.

530
00:41:05 --> 00:41:09
I love this term.


531
00:41:09 --> 00:41:13
A lurking variable is something
that actually effects

532
00:41:13 --> 00:41:18
the result, but is not
reflected in the model.

533
00:41:18 --> 00:41:26
As we'll see a little bit
later, this is a very important

534
00:41:26 --> 00:41:29
thing to worry about, when
you're looking at

535
00:41:29 --> 00:41:32
experimental data and you're
building models.

536
00:41:32 --> 00:41:36
So we see this, for example, in
the medical literature, that

537
00:41:36 --> 00:41:43
they will do some experiment,
and they'll say that this drug

538
00:41:43 --> 00:41:46
explains x, or has this affect.


539
00:41:46 --> 00:41:49
And the variables they are
looking at are, say, the

540
00:41:49 --> 00:41:55
disease the patient has, and
the age of the patient.

541
00:41:55 --> 00:42:00
Well, maybe the gender of the
patient is also important,

542
00:42:00 --> 00:42:04
but it doesn't happen
to be in the model.

543
00:42:04 --> 00:42:09
Now, if when they did a fit, it
came out with 0.9, that says at

544
00:42:09 --> 00:42:13
worst case, the variables
we didn't consider could

545
00:42:13 --> 00:42:19
cause a 10 percent error.


546
00:42:19 --> 00:42:23
But, that could be big,
that could matter a lot.

547
00:42:23 --> 00:42:29
And so as you get farther from
1, you ought to get very

548
00:42:29 --> 00:42:33
worried about whether you
actually have all the

549
00:42:33 --> 00:42:35
right variables.


550
00:42:35 --> 00:42:37
Now you might have the right
variables, and just experiment

551
00:42:37 --> 00:42:42
was not conducted well, But
it's usually the case that the

552
00:42:42 --> 00:42:46
problem is not that, but that
there are lurking variables.

553
00:42:46 --> 00:42:49
And we'll see examples of that.


554
00:42:49 --> 00:42:52
So, easier to read than the
math, at least by me, easier to

555
00:42:52 --> 00:43:07
read than the math, is the
implementation of r square.

556
00:43:07 --> 00:43:12
So it's measured and estimated
values, I get the diffs, the

557
00:43:12 --> 00:43:15
differences, between the
estimated and the measured.

558
00:43:15 --> 00:43:19
These are both arrays, so I
subtract 1 array from the

559
00:43:19 --> 00:43:20
other, and then I square it.


560
00:43:20 --> 00:43:24
Remember, this'll do an
element-wise subtraction, and

561
00:43:24 --> 00:43:26
then square each element.


562
00:43:26 --> 00:43:32
Then I can get the mean, by
dividing the sum of the array

563
00:43:32 --> 00:43:38
measured by the length of it.


564
00:43:38 --> 00:43:42
I can get the variance, which
is the measured mean minus the

565
00:43:42 --> 00:43:46
measured value, again squared.


566
00:43:46 --> 00:43:53
And then I'll return
1 minus this.

567
00:43:53 --> 00:43:55
All right?


568
00:43:55 --> 00:43:59
So, just to make sure we sort
of understand the code, and the

569
00:43:59 --> 00:44:05
theory here as well, what would
we get if we had absolutely

570
00:44:05 --> 00:44:08
perfect prediction?


571
00:44:08 --> 00:44:11
So if every measured point
actually fit on the curb

572
00:44:11 --> 00:44:19
predicted by our model, what
would r square return?

573
00:44:19 --> 00:44:24
So in this case, measured and
estimated would be identical.

574
00:44:24 --> 00:44:30
What gets return by this?


575
00:44:30 --> 00:44:32
Yeah, 1.


576
00:44:32 --> 00:44:38
Exactly right.


577
00:44:38 --> 00:44:43
Because when I compute it, it
will turn out that these two

578
00:44:43 --> 00:44:50
numbers will be the, I'll get
0, 1 minus 0 is 0, right?

579
00:44:50 --> 00:44:55
Because the differences
will be zero.

580
00:44:55 --> 00:44:59
OK?


581
00:44:59 --> 00:45:04
So I can use this, now,
to actually get a notion

582
00:45:04 --> 00:45:08
of how good my fit is.


583
00:45:08 --> 00:45:13
So let's look at speed dot pi
again here, and now I'm going

584
00:45:13 --> 00:45:17
to uncomment these two things,
where I'm going to, after I

585
00:45:17 --> 00:45:30
compute the fit, I'm going
to then measure it.

586
00:45:30 --> 00:45:34
And you'll see here that the r
squared error for the linear

587
00:45:34 --> 00:45:42
fit is 0.896, and for the
quadratic fit is 0.973.

588
00:45:42 --> 00:45:47
So indeed, we get a
much better fit here.

589
00:45:47 --> 00:45:51
So not only does our eye tell
us we have a better fit, our

590
00:45:51 --> 00:45:56
more formal statistical measure
tells us we have a better fit,

591
00:45:56 --> 00:45:57
and it tells us how good it is.


592
00:45:57 --> 00:46:02
It's not a perfect fit,
but it's a pretty

593
00:46:02 --> 00:46:07
good fit, for sure.


594
00:46:07 --> 00:46:13
Now, interestingly enough, it
isn't surprising that the

595
00:46:13 --> 00:46:20
quadratic fit is better
than the linear fit.

596
00:46:20 --> 00:46:24
In fact, the mathematics
of this should tell us

597
00:46:24 --> 00:46:28
it can never be worse.


598
00:46:28 --> 00:46:31
How do I know it can
never be worse?

599
00:46:31 --> 00:46:35
That's just, never is
a really strong word.

600
00:46:35 --> 00:46:38
How do I know that?


601
00:46:38 --> 00:46:42
Because, when I do the
quadratic fit, if I had

602
00:46:42 --> 00:46:47
perfectly linear data, then
this coefficient, whoops, not

603
00:46:47 --> 00:46:56
that coefficient, wrong, this
coefficient, could be 0.

604
00:46:56 --> 00:47:01
So if I ask it to do a
quadratic fit to linear data,

605
00:47:01 --> 00:47:06
and the a is truly perfectly
linear, this coefficient will

606
00:47:06 --> 00:47:09
be 0, and my model will turn
out to be the same as

607
00:47:09 --> 00:47:12
the linear model.


608
00:47:12 --> 00:47:19
So I will always get at
least as good a fit.

609
00:47:19 --> 00:47:24
Now, does this mean that
it's always better to use

610
00:47:24 --> 00:47:27
a higher order polynomial?


611
00:47:27 --> 00:47:38
The answer is no, and
let's look at why.

612
00:47:38 --> 00:47:48
So here what I've done is, I've
taken seven points, and I've

613
00:47:48 --> 00:47:54
generated, if you look at this
line here, the y-values, for x

614
00:47:54 --> 00:48:00
in x vals, points dot append
x plus some random number.

615
00:48:00 --> 00:48:04
So basically I've got something
linear in x, but I'm

616
00:48:04 --> 00:48:08
perturbing, if you will, my
data by some random value.

617
00:48:08 --> 00:48:11
Something between 0 and 1 is
getting added to things.

618
00:48:11 --> 00:48:14
And I'm doing this so my points
won't lie on a perfectly

619
00:48:14 --> 00:48:19
straight line.


620
00:48:19 --> 00:48:24
And then we'll try and
fit a line to it.

621
00:48:24 --> 00:48:28
And also, just for fun,
we'll try and fit a fifth

622
00:48:28 --> 00:48:30
order polynomial to it.


623
00:48:30 --> 00:48:40
And let's see what we get.


624
00:48:40 --> 00:48:43
Well, there's my line,
and there's my fifth

625
00:48:43 --> 00:48:45
order polynomial.


626
00:48:45 --> 00:48:49
Neither is quite perfect, but
which do you think looks

627
00:48:49 --> 00:48:53
like a closer fit?


628
00:48:53 --> 00:49:00
With your eye.


629
00:49:00 --> 00:49:06
Well, I would say the red line,
the red curve, if you will, is

630
00:49:06 --> 00:49:11
a better fit, and sure enough
if we look at the statistics,

631
00:49:11 --> 00:49:16
we'll see it's 0.99,
as opposed to 0.978.

632
00:49:16 --> 00:49:21
So it's clearly a closer fit.


633
00:49:21 --> 00:49:30
But that raises the very
important question: does closer

634
00:49:30 --> 00:49:40
equal better, or tighter, which
is another word for closer?

635
00:49:40 --> 00:49:44
And the answer is no.


636
00:49:44 --> 00:49:49
It's a tighter fit, but it's
not necessarily better, in

637
00:49:49 --> 00:49:52
the sense of more useful.


638
00:49:52 --> 00:49:55
Because one of the things I
want to do when I build a model

639
00:49:55 --> 00:50:00
like this, is have something
with predictive power.

640
00:50:00 --> 00:50:05
I don't really necessarily need
a model to tell me where the

641
00:50:05 --> 00:50:08
points I've measured lie,
because I have them.

642
00:50:08 --> 00:50:12
The whole purpose of the model
is to give me some way to

643
00:50:12 --> 00:50:17
predict where unmeasured points
would lie, where future

644
00:50:17 --> 00:50:19
points would lie.


645
00:50:19 --> 00:50:24
OK, I understand how the spring
works, and I can guess where it

646
00:50:24 --> 00:50:27
would be if things I haven't
had the time to measure, or

647
00:50:27 --> 00:50:31
the ability to measure.


648
00:50:31 --> 00:50:38
So let's look at that.


649
00:50:38 --> 00:50:41
Let's see, where'd
that figure go.

650
00:50:41 --> 00:50:47
It's lurking somewhere.


651
00:50:47 --> 00:50:54
All right, we'll just
kill this for now.

652
00:50:54 --> 00:51:00
So let's generate some more
points, and I'm going to use

653
00:51:00 --> 00:51:05
exactly the same algorithm.


654
00:51:05 --> 00:51:09
But I'm going to generate
twice as many points.

655
00:51:09 --> 00:51:14
But I'm only fitting
it to the first half.

656
00:51:14 --> 00:51:24
So if I run this one,
figure one is what

657
00:51:24 --> 00:51:26
we looked at before.


658
00:51:26 --> 00:51:29
The red line is fitting
them a little better.

659
00:51:29 --> 00:51:33
But here's figure two.


660
00:51:33 --> 00:51:39
What happens when I extrapolate
the curve to the new points?

661
00:51:39 --> 00:51:43
Well, you can see,
it's a terrible fit.

662
00:51:43 --> 00:51:46
And you would expect that,
because my data was basically

663
00:51:46 --> 00:51:52
linear, and I fit in
non-linear curve to it.

664
00:51:52 --> 00:51:56
And if you look at it you can
see that, OK, look at this, to

665
00:51:56 --> 00:51:59
get from here to here, it
thought I had to take

666
00:51:59 --> 00:52:02
off pretty sharply.


667
00:52:02 --> 00:52:06
And so sure enough, as I get
new points, the prediction will

668
00:52:06 --> 00:52:11
postulate that it's still going
up, much more steeply

669
00:52:11 --> 00:52:14
than it really does.


670
00:52:14 --> 00:52:18
So you can see it's a
terrible prediction.

671
00:52:18 --> 00:52:28
And that's because what I've
done is, I over-fit the data.

672
00:52:28 --> 00:52:32
I've taken a very high degree
polynomial, which has given me

673
00:52:32 --> 00:52:36
a good close fit, and I can
always get a fit, by the way.

674
00:52:36 --> 00:52:40
If I choose a high enough
degree polynomial, I can fit

675
00:52:40 --> 00:52:43
lots and lots of data sets.


676
00:52:43 --> 00:52:47
But I have reason to
be very suspicious.

677
00:52:47 --> 00:52:49
The fact that I took a fifth
order polynomial to get six

678
00:52:49 --> 00:52:57
points should make
me very nervous.

679
00:52:57 --> 00:52:59
And it's a very
important moral.

680
00:52:59 --> 00:53:01
Beware of over-fitting.


681
00:53:01 --> 00:53:07
If you have a very complex
model, there's a good

682
00:53:07 --> 00:53:12
chance that it's over-fit.


683
00:53:12 --> 00:53:18
The larger moral is, beware of
statistics without any theory.

684
00:53:18 --> 00:53:21
You're just cranking away, you
get a great r squared, you

685
00:53:21 --> 00:53:23
say it's a beautiful fit.


686
00:53:23 --> 00:53:26
But there was no
real theory there.

687
00:53:26 --> 00:53:29
You can always find a fit.


688
00:53:29 --> 00:53:32
As Disraeli is alleged to have
said, there are three kinds

689
00:53:32 --> 00:53:38
of lies: lies, damned
lies, and statistics.

690
00:53:38 --> 00:53:41
And we'll spend some more
time when we get back from

691
00:53:41 --> 00:53:44
Thanksgiving looking at how
to lie with statistics.

692
00:53:44 --> 00:53:46
Have a great
holiday, everybody.

693
00:53:46 --> 00:53:47



