1
00:00:00 --> 00:00:00



2
00:00:00 --> 00:00:02
The following
content is provided under a

3
00:00:02 --> 00:00:03
Creative Commons license.


4
00:00:03 --> 00:00:06
You're support will help MIT
OpenCourseWare continue to

5
00:00:06 --> 00:00:10
offer high quality educational
resources for free.

6
00:00:10 --> 00:00:13
To make a donation or view
additional materials from

7
00:00:13 --> 00:00:16
hundreds of MIT courses,
visit MIT OpenCourseWare

8
00:00:16 --> 00:00:19
course at ocw.mit.edu.


9
00:00:19 --> 00:00:21
PROFESSOR: All right.


10
00:00:21 --> 00:00:24
Today's lecture, mostly I
want to talk about what

11
00:00:24 --> 00:00:27
computer scientists do.


12
00:00:27 --> 00:00:31
We've sort of been teaching you
computer science in the small,

13
00:00:31 --> 00:00:34
I want to pull back and think
in the large about, what do

14
00:00:34 --> 00:00:36
people do once they learn
about computer science?

15
00:00:36 --> 00:00:40
And then I'll wrap up with a
quick overview of what I think

16
00:00:40 --> 00:00:42
we've accomplished this term.


17
00:00:42 --> 00:00:45
So what does a computer
scientist do?

18
00:00:45 --> 00:00:48
What they really do is,
almost everything.

19
00:00:48 --> 00:00:51
Graduates of our department,
other departments, have done

20
00:00:51 --> 00:00:55
things like animation for
movies you've all seen, they

21
00:00:55 --> 00:00:59
keep airplanes from falling out
of the sky, they help surgeons

22
00:00:59 --> 00:01:01
do a better job of brain
surgery, that's something

23
00:01:01 --> 00:01:04
Professor Grimson
has worked on.

24
00:01:04 --> 00:01:10
All sorts of exciting things.


25
00:01:10 --> 00:01:13
Mostly what they have in
common, and it's incredibly

26
00:01:13 --> 00:01:17
varied, what you can do with
computer science, but it all

27
00:01:17 --> 00:01:20
involves thinking
computationally.

28
00:01:20 --> 00:01:25
And if I had to use a single
phrase to describe 6.00, that's

29
00:01:25 --> 00:01:27
the phrase I would use.


30
00:01:27 --> 00:01:31
The semester is really about
computational thinking.

31
00:01:31 --> 00:01:34
I think this will be a
fundamental skill used by

32
00:01:34 --> 00:01:39
everyone in the world by the
middle of the current century.

33
00:01:39 --> 00:01:43
And it'll be like the, it will
become one of the three r's,

34
00:01:43 --> 00:01:47
reading writing arithmetic.


35
00:01:47 --> 00:01:49
And I haven't quite figured I
how to put an r in the front

36
00:01:49 --> 00:01:53
of it, but sooner or later.


37
00:01:53 --> 00:01:54
So what is it?


38
00:01:54 --> 00:01:57
Well, the process is the
process that we've sort

39
00:01:57 --> 00:02:01
of been advocating
throughout the semester.

40
00:02:01 --> 00:02:04
You identify or invent
useful abstractions.

41
00:02:04 --> 00:02:08
Sometimes they're out there,
and we can just pluck them.

42
00:02:08 --> 00:02:12
Sometimes you have to
invent them ourselves.

43
00:02:12 --> 00:02:15
We then formulate a solution to
a problem as some sort of a

44
00:02:15 --> 00:02:19
computational experiment.


45
00:02:19 --> 00:02:22
Typically using
those abstractions.

46
00:02:22 --> 00:02:26
Then comes the part that many
you get bogged down in, but

47
00:02:26 --> 00:02:29
it'll get easier for
you as you go on.

48
00:02:29 --> 00:02:33
Is to design and construct
a sufficiently efficient

49
00:02:33 --> 00:02:36
implementation of
the experiment.

50
00:02:36 --> 00:02:40
And I want to emphasize the
word sufficiently here, in that

51
00:02:40 --> 00:02:43
it only has to be fast enough
to run it and get an answer.

52
00:02:43 --> 00:02:48
And it doesn't have to be
the fastest possible.

53
00:02:48 --> 00:02:52
We then need to validate the
experimental setup, convince

54
00:02:52 --> 00:02:55
ourselves that the code is
right, and then run the

55
00:02:55 --> 00:02:59
experiment, and then evaluate
the results, and then

56
00:02:59 --> 00:03:01
repeat as needed.


57
00:03:01 --> 00:03:03
And that's, of course, the key.


58
00:03:03 --> 00:03:07
By now you've all learned that
these experiments rarely are

59
00:03:07 --> 00:03:10
done the first time
you run them.

60
00:03:10 --> 00:03:12
But of course, this is true of
experiments in biology and

61
00:03:12 --> 00:03:16
physics and chemistry
and everything else.

62
00:03:16 --> 00:03:18
But this is the basic process.


63
00:03:18 --> 00:03:24
And I think, it's a view that's
not universally held, but I

64
00:03:24 --> 00:03:26
think is the correct view
of what computational

65
00:03:26 --> 00:03:29
thinking is all about.


66
00:03:29 --> 00:03:33
It's an experimental
discipline.

67
00:03:33 --> 00:03:38
So the two a's are abstraction,
choosing the right abstraction,

68
00:03:38 --> 00:03:42
and operating in terms
of multiple layers of

69
00:03:42 --> 00:03:45
abstraction simultaneously.


70
00:03:45 --> 00:03:47
And that's a little bit tricky.


71
00:03:47 --> 00:03:50
Something that you've
worked on this term.

72
00:03:50 --> 00:03:56
You invent some beautiful,
high level of abstraction.

73
00:03:56 --> 00:03:58
But then you have to think
about it, and implement it

74
00:03:58 --> 00:04:01
using the rather low level
abstractions provided by

75
00:04:01 --> 00:04:04
the programming language.


76
00:04:04 --> 00:04:10
So we'll do a simulation of,
well, the stock market, we've

77
00:04:10 --> 00:04:12
looked at it just recently.


78
00:04:12 --> 00:04:16
And you've got this abstraction
of a stock, and at one level,

79
00:04:16 --> 00:04:18
when we're thinking about the
market, we think of a stock

80
00:04:18 --> 00:04:20
abstractly, as something
that has a price that

81
00:04:20 --> 00:04:22
moves up and down.


82
00:04:22 --> 00:04:25
But then at the same time, we
say, well, is the movement

83
00:04:25 --> 00:04:29
Gaussian, or uniform, and
we're operating at a

84
00:04:29 --> 00:04:31
somewhat different level.


85
00:04:31 --> 00:04:34
And then at a lower level,
we're asking questions

86
00:04:34 --> 00:04:37
like, well, how should
we represent it? is?

87
00:04:37 --> 00:04:41
It a dictionary, is it a
list, that sort of thing.

88
00:04:41 --> 00:04:46
And we always bounce back and
forth and spend a lot of time

89
00:04:46 --> 00:04:49
thinking about how
the layers relate.

90
00:04:49 --> 00:04:53
Now the other thing that's
important, and it really

91
00:04:53 --> 00:04:55
distinguishes, I think,
computing from a lot

92
00:04:55 --> 00:04:57
of other disciplines.


93
00:04:57 --> 00:05:02
Is, we don't just write elegant
descriptions of abstractions

94
00:05:02 --> 00:05:07
on paper, the way you say,
would do in mathematics.

95
00:05:07 --> 00:05:11
But we have to then automate
those attractions.

96
00:05:11 --> 00:05:13
And we always think in terms
of, how can we mechanize

97
00:05:13 --> 00:05:15
the abstractions?


98
00:05:15 --> 00:05:19
And that's the computational
part, in many ways.

99
00:05:19 --> 00:05:21
How can we do that?


100
00:05:21 --> 00:05:25
Well, we can do it because we
have precise and exacting

101
00:05:25 --> 00:05:29
notations in which to
express these models.

102
00:05:29 --> 00:05:32
So I'll say a few words about
the model of stocks, or a few

103
00:05:32 --> 00:05:35
words about the model of a
drunk wandering around a

104
00:05:35 --> 00:05:40
field, but I can't get
away with just the words.

105
00:05:40 --> 00:05:44
Eventually I have to
convert the words to code.

106
00:05:44 --> 00:05:47
And that's sort of the
acid test of truth.

107
00:05:47 --> 00:05:50
Have I really understood
what the words mean?

108
00:05:50 --> 00:05:54
Because if I don't, I can't
actually convert it to code.

109
00:05:54 --> 00:05:57
And that's both the cursing
and the bless of being

110
00:05:57 --> 00:05:59
a computer scientist.


111
00:05:59 --> 00:06:05
It's the curse because,
you can't fake it, right?

112
00:06:05 --> 00:06:08
And that's one of the reasons
that many students end up

113
00:06:08 --> 00:06:12
spending a lot of time
in a course like 6.00.

114
00:06:12 --> 00:06:17
Because they know whether or
not their program works.

115
00:06:17 --> 00:06:20
When you write a proof in math,
you can delude we into thinking

116
00:06:20 --> 00:06:23
it's a solid proof,
when it isn't.

117
00:06:23 --> 00:06:26
When you write an essay, you
can delude yourself into

118
00:06:26 --> 00:06:28
thinking it's brilliant,
when it isn't.

119
00:06:28 --> 00:06:32
But here, you look at what it
does, and it either did what

120
00:06:32 --> 00:06:34
you thought it would
do or didn't.

121
00:06:34 --> 00:06:38
And so that means you sometimes
have to work extra hard, but

122
00:06:38 --> 00:06:41
the blessing is, when it's
done, you really know

123
00:06:41 --> 00:06:43
you've done it.


124
00:06:43 --> 00:06:45
And you've got
something useful.

125
00:06:45 --> 00:06:49
And that's because there's a
machine that will execute

126
00:06:49 --> 00:06:51
your descriptions.


127
00:06:51 --> 00:06:55
And that's what makes it fun.


128
00:06:55 --> 00:06:58
So some examples of
computational thinking.

129
00:06:58 --> 00:07:00
How difficult is this problem?


130
00:07:00 --> 00:07:02
That's what we talked
about when we talked

131
00:07:02 --> 00:07:04
about complexity.


132
00:07:04 --> 00:07:08
And we talked about, for many
problems, for all problems,

133
00:07:08 --> 00:07:11
there's an intrinsic difficulty
of solving the problem

134
00:07:11 --> 00:07:13
with the computation.


135
00:07:13 --> 00:07:18
Independent of how efficient
your particular solution is.

136
00:07:18 --> 00:07:21
And then the question is,
how can I best solve it?

137
00:07:21 --> 00:07:25
Theoretical computer science
has given us precise meaning to

138
00:07:25 --> 00:07:29
these and related questions.


139
00:07:29 --> 00:07:32
Thinking recursively,
another good example.

140
00:07:32 --> 00:07:36
Where we take a seemingly
difficult problem, and

141
00:07:36 --> 00:07:41
reformulate it into one
which we know how to solve.

142
00:07:41 --> 00:07:44
Very often, it's a
smaller instance of

143
00:07:44 --> 00:07:46
the original problem.


144
00:07:46 --> 00:07:49
And we say, gosh, this
is hard to solve.

145
00:07:49 --> 00:07:51
But then we think, well suppose
the list only had two elements

146
00:07:51 --> 00:07:53
in it, would I know
how to sort it?

147
00:07:53 --> 00:07:55
Yeah.


148
00:07:55 --> 00:07:57
Well, then we can build up
and say, therefore I can

149
00:07:57 --> 00:08:02
use that idea to sort
a list of any size.

150
00:08:02 --> 00:08:05
As part of this thinking
recursively we learn about

151
00:08:05 --> 00:08:11
reduction, we learned how to,
say, reduce the problem of

152
00:08:11 --> 00:08:16
deciding what courses to take
to an optimization problem in

153
00:08:16 --> 00:08:18
terms of the knapsack problem.


154
00:08:18 --> 00:08:20
Once we did that, we could say,
oh, we know how to solve all

155
00:08:20 --> 00:08:23
knapsack problems, dynamic
programming has been around a

156
00:08:23 --> 00:08:27
long time, we'll just do that.


157
00:08:27 --> 00:08:30
So we do a lot of these
reductions, in betting,

158
00:08:30 --> 00:08:33
transformations, transforming
one problem into another,

159
00:08:33 --> 00:08:36
and simulations.


160
00:08:36 --> 00:08:39
So some other examples,
choosing an appropriate

161
00:08:39 --> 00:08:43
representation, modeling the
relevant abstracts of a

162
00:08:43 --> 00:08:46
problem to make it practical.


163
00:08:46 --> 00:08:48
And this is the essence of
abstraction that computer

164
00:08:48 --> 00:08:53
scientists learn, is how to
figure out what's relevant and

165
00:08:53 --> 00:08:57
ignore what is irrelevant,
or less relevant.

166
00:08:57 --> 00:08:59
Focus on the important things.


167
00:08:59 --> 00:09:05
That's an enormous skill in not
only computing, but in life.

168
00:09:05 --> 00:09:09
We worry about worst-case
scenarios, damage containment,

169
00:09:09 --> 00:09:12
error correction, all of this
we talked about, we talked

170
00:09:12 --> 00:09:16
about debugging, defensive
programming, making sure the

171
00:09:16 --> 00:09:19
types were right, but again
these are very general

172
00:09:19 --> 00:09:21
things that we think about.


173
00:09:21 --> 00:09:25
And we think about the fact,
sometimes, it's kind of a nice

174
00:09:25 --> 00:09:29
idea, the fact that there
exists really hard problems

175
00:09:29 --> 00:09:32
is actually a good thing.


176
00:09:32 --> 00:09:35
Because that's what let's us do
things like use encryption to

177
00:09:35 --> 00:09:40
get privacy of our data, or
privacy for our communications,

178
00:09:40 --> 00:09:42
privacy of our phone calls.


179
00:09:42 --> 00:09:45
Because we have learned that
some problems are intrinsically

180
00:09:45 --> 00:09:50
difficult to solve, and that's
what coding is all about.

181
00:09:50 --> 00:09:52
Or encryption is all about.


182
00:09:52 --> 00:09:54
That's a nice thing.


183
00:09:54 --> 00:09:59
I now want to talk more
specifically about what

184
00:09:59 --> 00:10:02
one group of computer
scientists does.

185
00:10:02 --> 00:10:04
And that's my research group.


186
00:10:04 --> 00:10:09
Which consists of mostly
graduate students, but every

187
00:10:09 --> 00:10:13
year I try and incorporate a
few undergraduates in it.

188
00:10:13 --> 00:10:15
We work in the area of
medicine, it's also the area

189
00:10:15 --> 00:10:19
in which Professor Grimson
does most of his work.

190
00:10:19 --> 00:10:22
The goal is pretty simple:
we want to help people live

191
00:10:22 --> 00:10:25
longer and better lives.


192
00:10:25 --> 00:10:26
Right?


193
00:10:26 --> 00:10:31
I don't think any of you will
think that's a worthless goal.

194
00:10:31 --> 00:10:33
But, a second goal is,
we want to have fun

195
00:10:33 --> 00:10:35
while we're doing it.


196
00:10:35 --> 00:10:39
Because life is way too
short to do anything that's

197
00:10:39 --> 00:10:41
not fun for very long.


198
00:10:41 --> 00:10:44
So we push the frontiers of
computer science, electrical

199
00:10:44 --> 00:10:46
engineering, and medicine.


200
00:10:46 --> 00:10:50
Working in close collaboration
with physicians, and here

201
00:10:50 --> 00:10:54
you've got logos of some of the
hospitals with which we work.

202
00:10:54 --> 00:10:57
It is, of course,
fairly technical work.

203
00:10:57 --> 00:11:00
We use machine learning,
clustering, data mining,

204
00:11:00 --> 00:11:03
algorithm design,
signal processing.

205
00:11:03 --> 00:11:09
And everything we do ends up,
eventually, getting translated

206
00:11:09 --> 00:11:13
to code which we then run.


207
00:11:13 --> 00:11:15
And so we worry a lot
about software systems

208
00:11:15 --> 00:11:17
and the quality.


209
00:11:17 --> 00:11:21
We do write systems that, for
example, inject electrical

210
00:11:21 --> 00:11:24
signals into people's brains.


211
00:11:24 --> 00:11:25
And it's kind of important,
when you do that, that

212
00:11:25 --> 00:11:28
the software does what
it's intended to do.

213
00:11:28 --> 00:11:31
At least if you
like the people.

214
00:11:31 --> 00:11:36
So some specific activities,
and this is only a sampling.

215
00:11:36 --> 00:11:39
We work on extracting
clinically useful information

216
00:11:39 --> 00:11:43
from electrical signals
in the human body.

217
00:11:43 --> 00:11:46
Mostly the heart, the brain,
and connected anatomy.

218
00:11:46 --> 00:11:49
For those of you who've never
studied anatomy, I've labeled

219
00:11:49 --> 00:11:53
the heart and the brain
in the diagram for you.

220
00:11:53 --> 00:11:57
The two major projects in this
area we've been working on, is

221
00:11:57 --> 00:12:02
predicting adverse cardiac
events and detecting

222
00:12:02 --> 00:12:06
and responding to
epileptic seizures.

223
00:12:06 --> 00:12:11
And I want to say a few
words about each of those.

224
00:12:11 --> 00:12:13
We'll skip this.


225
00:12:13 --> 00:12:19
So example one,
treating epilepsy.

226
00:12:19 --> 00:12:22
I suspect most of you are
surprised by the fact that

227
00:12:22 --> 00:12:27
one percent of the world's
population has epilepsy.

228
00:12:27 --> 00:12:30
This is true in almost
every part of the world.

229
00:12:30 --> 00:12:33
It's true in the United States,
it's true in underdeveloped

230
00:12:33 --> 00:12:35
countries in Central Africa.


231
00:12:35 --> 00:12:39
It seems to be one of the few
diseases that's invariant

232
00:12:39 --> 00:12:43
to economic situations.


233
00:12:43 --> 00:12:47
One of the reasons that people
are surprised by this number

234
00:12:47 --> 00:12:49
is, most folks who
have epilepsy try to

235
00:12:49 --> 00:12:52
keep it a secret.


236
00:12:52 --> 00:12:55
There's a long history, getting
back to witches, people with

237
00:12:55 --> 00:12:58
epilepsy were deemed to be
witches, were burned at the

238
00:12:58 --> 00:13:02
stake, things of that nature.


239
00:13:02 --> 00:13:06
People who have epilepsy are
not allowed by the FAA to work

240
00:13:06 --> 00:13:10
on maintenance of aircraft.


241
00:13:10 --> 00:13:15
It's a ridiculous restriction,
but it's in the law, so I know

242
00:13:15 --> 00:13:18
someone with epilepsy who's
kept it a secret because he

243
00:13:18 --> 00:13:21
doesn't want to lose his job.


244
00:13:21 --> 00:13:23
That's a different sermon.


245
00:13:23 --> 00:13:29
It's characterized by recurrent
seizures, generated by abnormal

246
00:13:29 --> 00:13:33
electrical activity
in the brain.

247
00:13:33 --> 00:13:38
It's really less a disease than
a description of symptoms,

248
00:13:38 --> 00:13:41
because there are lots of
independent, different

249
00:13:41 --> 00:13:43
causes of it.


250
00:13:43 --> 00:13:47
It can be inherited, that's
a significant fraction.

251
00:13:47 --> 00:13:51
Or it can be acquired,
typically by some sort

252
00:13:51 --> 00:13:54
of insult to the brain.


253
00:13:54 --> 00:13:59
People who have strokes
or serious infections

254
00:13:59 --> 00:14:00
will develop epilepsy.


255
00:14:00 --> 00:14:06
Hemorrhages, and
increasingly head injuries.

256
00:14:06 --> 00:14:09
Turns out to be an illness that
is of now great interested the

257
00:14:09 --> 00:14:12
Defense Department because
people are coming back

258
00:14:12 --> 00:14:16
from Iraq with epilepsy.


259
00:14:16 --> 00:14:19
So those of you who don't have
a strong stomach, you might

260
00:14:19 --> 00:14:21
want to avert your eyes.


261
00:14:21 --> 00:14:23
Here's one manifestation of it.


262
00:14:23 --> 00:14:27
On the left is an EEG, and
that's, in this case, a

263
00:14:27 --> 00:14:30
recording of the electrical
activity in the surface

264
00:14:30 --> 00:14:32
of this girl's brain.


265
00:14:32 --> 00:14:34
You can see she's wearing
what looks like a very

266
00:14:34 --> 00:14:36
funny looking shower cap.


267
00:14:36 --> 00:14:38
That's where the
electrodes are.

268
00:14:38 --> 00:14:41
And it's recording the
electrical activity in

269
00:14:41 --> 00:14:44
the brain, on the right
is obviously a video.

270
00:14:44 --> 00:14:46
Well, pretty frightening.


271
00:14:46 --> 00:14:48
Not all seizures are
quite that frightening.

272
00:14:48 --> 00:14:50
In fact, most of them are not.


273
00:14:50 --> 00:14:53
Sometimes people have what are
called absence seizures, where

274
00:14:53 --> 00:14:58
they just sort of black
out for a period of time.

275
00:14:58 --> 00:15:00
Still not a good thing if
you're, say, driving a

276
00:15:00 --> 00:15:03
car when that happens.


277
00:15:03 --> 00:15:07
The key issue here, is that
the onset of the seizure

278
00:15:07 --> 00:15:10
seems unpredictable.


279
00:15:10 --> 00:15:15
And in particular, it seems
unpredictable to the person

280
00:15:15 --> 00:15:17
who has the seizure.


281
00:15:17 --> 00:15:19
And that's the real risk.


282
00:15:19 --> 00:15:22
Seizures are typically
self-limiting.

283
00:15:22 --> 00:15:25
They will correct
themselves over time.

284
00:15:25 --> 00:15:31
Eventually this girl will
return to, quote, normal.

285
00:15:31 --> 00:15:33
The seizure itself will
probably be over in a minute

286
00:15:33 --> 00:15:37
or two, and for about an
hour she'll be confused, and

287
00:15:37 --> 00:15:39
then she'll be OK again.


288
00:15:39 --> 00:15:42
But, because they're
unpredictable, even people who

289
00:15:42 --> 00:15:50
have as few as, say, two a
year, it dominates their life.

290
00:15:50 --> 00:15:52
So, if you're that girl, and
even if you're going to

291
00:15:52 --> 00:15:55
have one of those a year,
you're never going to be

292
00:15:55 --> 00:15:57
allowed on a bicycle.


293
00:15:57 --> 00:15:59
Because if that happens
when you're on a bicycle,

294
00:15:59 --> 00:16:01
it's really bad.


295
00:16:01 --> 00:16:03
If you're an adult, and you
have one or two year, you

296
00:16:03 --> 00:16:04
should never drive a car.


297
00:16:04 --> 00:16:10
Imagine if you've been driving
a car when that happened.

298
00:16:10 --> 00:16:13
Almost everybody with
epilepsy eventually

299
00:16:13 --> 00:16:15
suffers a serious injury.


300
00:16:15 --> 00:16:17
They have a seizure while
they're on a flight of stairs,

301
00:16:17 --> 00:16:20
and they fall down and
fracture their skull.

302
00:16:20 --> 00:16:23
They get intra-cranial
hematomas, they get burns if

303
00:16:23 --> 00:16:27
they're cooking at the stove,
they drown in the bathtub.

304
00:16:27 --> 00:16:32
And it's the unpredictability
that's bad.

305
00:16:32 --> 00:16:34
Death is high, two to
three times out of the

306
00:16:34 --> 00:16:35
general population.


307
00:16:35 --> 00:16:40
Typically by accident
related to the seizure.

308
00:16:40 --> 00:16:45
And then there's something
called sudden unexplained death

309
00:16:45 --> 00:16:52
in epilepsy patients, SUDEP,
which is estimated to be one

310
00:16:52 --> 00:16:55
per 100 patients per year.


311
00:16:55 --> 00:16:58
Just will die, and
no one knows why.

312
00:16:58 --> 00:17:02
Frequently it will be at night,
while they're in bed, and the

313
00:17:02 --> 00:17:05
conjecture is, they have a
seizure, and end up face down

314
00:17:05 --> 00:17:08
in the pillow, smother,
that kind of thing.

315
00:17:08 --> 00:17:10
So you can imagine, if you're a
parent, you have a child with

316
00:17:10 --> 00:17:13
this, you don't sleep much.


317
00:17:13 --> 00:17:15
Really very sad.


318
00:17:15 --> 00:17:19
So we want to do
something about that.

319
00:17:19 --> 00:17:23
And our idea is to detect
the seizure early.

320
00:17:23 --> 00:17:28
There are two onset times,
electrographic and clinical.

321
00:17:28 --> 00:17:31
Electrographic is when the
brain first starts looking

322
00:17:31 --> 00:17:36
suspicious, and clinical is
when there's a overt physical

323
00:17:36 --> 00:17:40
symptom, that stiffness you
saw in that young girl.

324
00:17:40 --> 00:17:44
Probably it went by too fast
for any of you to notice, but

325
00:17:44 --> 00:17:47
if you'd looked carefully at
the EEG on the left of that

326
00:17:47 --> 00:17:53
picture, you would have seen
it going abnormal before she

327
00:17:53 --> 00:17:56
had any clinical symptoms.


328
00:17:56 --> 00:17:59
Because the symptoms are caused
by the electrical activity in

329
00:17:59 --> 00:18:03
the brain, by definition, it
proceeds the symptoms, or at

330
00:18:03 --> 00:18:05
least doesn't follow them.


331
00:18:05 --> 00:18:08
So our idea was to
try and detect the

332
00:18:08 --> 00:18:11
electrographic onset.


333
00:18:11 --> 00:18:14
If you could do this, you
could provide warning.

334
00:18:14 --> 00:18:18
So you could tell somebody, sit
down before you fall down.

335
00:18:18 --> 00:18:19
Get out of the bathtub.


336
00:18:19 --> 00:18:22
Back away from the stove.


337
00:18:22 --> 00:18:25
Just an oral warning would
be tremendously useful.

338
00:18:25 --> 00:18:27
You could summon help.


339
00:18:27 --> 00:18:31
You could tell a parent, come
quick, your child is having a

340
00:18:31 --> 00:18:34
seizure right now in
bed or elsewhere.

341
00:18:34 --> 00:18:39
There are fast-acting drugs,
Ativan, which if you inhale

342
00:18:39 --> 00:18:43
at the start of a seizure,
can stop it, reduce it.

343
00:18:43 --> 00:18:46
And our particular interest
is neural stimulation.

344
00:18:46 --> 00:18:51
There is reason to believe,
it's yet to be proved, that

345
00:18:51 --> 00:18:54
if you apply the correct
electrical stimulation at

346
00:18:54 --> 00:18:58
the very beginning of a
seizure, you can block it.

347
00:18:58 --> 00:19:02
Or at least ameliorate
it greatly.

348
00:19:02 --> 00:19:04
So, here's a picture of an EEG.


349
00:19:04 --> 00:19:08
And I'll just point out,
seizures can be tricky to find.

350
00:19:08 --> 00:19:11
You might think this is a
seizure, but it isn't.

351
00:19:11 --> 00:19:13
That's blinking your eyes.


352
00:19:13 --> 00:19:16
Oh well.


353
00:19:16 --> 00:19:21
Here is the seizure, but in
fact, it's begun about here.

354
00:19:21 --> 00:19:25
Where things don't look
particularly ominous.

355
00:19:25 --> 00:19:30
So it's pretty
subtle to detect.

356
00:19:30 --> 00:19:35
One of the problems is, EEG
varies across patients.

357
00:19:35 --> 00:19:38
And epileptics have
abnormal baselines.

358
00:19:38 --> 00:19:38
Yeah?


359
00:19:38 --> 00:19:48
STUDENT: [INAUDIBLE]


360
00:19:48 --> 00:19:50
PROFESSOR: Highly variable
in patients, but that's

361
00:19:50 --> 00:19:52
kind of typical.


362
00:19:52 --> 00:19:56
But that would be plenty of
time to back away from the

363
00:19:56 --> 00:19:59
stove, or get off a bicycle.


364
00:19:59 --> 00:20:02
Not so much time to get
off the Mass Turnpike,

365
00:20:02 --> 00:20:05
if you're driving a car.


366
00:20:05 --> 00:20:07
But it would be great,
if you can give 10 or

367
00:20:07 --> 00:20:08
15 seconds warning.


368
00:20:08 --> 00:20:11
In fact, if you could give five
seconds warning, it would

369
00:20:11 --> 00:20:14
be tremendously useful.


370
00:20:14 --> 00:20:18
And for the treatments, it's
even different, right?

371
00:20:18 --> 00:20:21
Yes, you're exactly right,
in that particular case.

372
00:20:21 --> 00:20:24
The problem is, one of the
things I've discovered in

373
00:20:24 --> 00:20:29
medicine, my work, is that
almost all healthy people

374
00:20:29 --> 00:20:34
look alike, and all sick
people look different.

375
00:20:34 --> 00:20:40
So, EEG, if you have epilepsy,
your quote, normal EEG, when

376
00:20:40 --> 00:20:44
you're not having a seizure,
looks weird compared to people

377
00:20:44 --> 00:20:48
who don't have epilepsy.


378
00:20:48 --> 00:20:51
People have been building
seizure detectors for about

379
00:20:51 --> 00:20:55
40 years now, and they
don't work very well.

380
00:20:55 --> 00:20:59
Every EEG that gets shipped
comes with a seizure detector,

381
00:20:59 --> 00:21:03
and the first thing most
hospitals do is, turn it off.

382
00:21:03 --> 00:21:07
Because it just doesn't work.


383
00:21:07 --> 00:21:11
The good news is, that even
though each epileptics brain

384
00:21:11 --> 00:21:15
signals look different, once
you've learned what an

385
00:21:15 --> 00:21:19
individual's signal looks
like, it's pretty consistent.

386
00:21:19 --> 00:21:22
Both the interictal,
between seizure, and the

387
00:21:22 --> 00:21:25
start of the seizure.


388
00:21:25 --> 00:21:30
Well, that tells a computer
scientist, let's use machine

389
00:21:30 --> 00:21:36
learning to figure out what
that patient's EEG looks like.

390
00:21:36 --> 00:21:41
And build a highly specific
detector that will work great

391
00:21:41 --> 00:21:45
for one person, and not
at all for anybody else.

392
00:21:45 --> 00:21:48
And then we'll just
install it, and you know,

393
00:21:48 --> 00:21:50
everyone gets their own.


394
00:21:50 --> 00:21:55
So instead of one size fits
all, it's design a detector

395
00:21:55 --> 00:21:58
to fit the patient.


396
00:21:58 --> 00:22:03
We have been working on this
now for about six years.

397
00:22:03 --> 00:22:09
We've done, we've looked at
about 80 patients now, more.

398
00:22:09 --> 00:22:12
And, highly successful.


399
00:22:12 --> 00:22:18
And I'm not going to weigh you
down with all the statistical

400
00:22:18 --> 00:22:20
tests and everything, but
we've done the kind of things

401
00:22:20 --> 00:22:21
I talked about in class.


402
00:22:21 --> 00:22:25
Looking at the data, doing the
statistics, and suffice it

403
00:22:25 --> 00:22:27
to say, it works great.


404
00:22:27 --> 00:22:31
For nine out of 10
patients, roughly.

405
00:22:31 --> 00:22:35
And for the tenth, we just have
to say, sorry, we can't do it.

406
00:22:35 --> 00:22:40
But, you know, you can't help
everybody all the time.

407
00:22:40 --> 00:22:42
We're currently, what you
see here, is a picture

408
00:22:42 --> 00:22:44
of a neural stimulator.


409
00:22:44 --> 00:22:49
The stimulator itself gets
implanted under the clavicle,

410
00:22:49 --> 00:22:52
and then a wire runs up and
wraps around the left vagus

411
00:22:52 --> 00:22:56
nerve, which is the longest
nerve in the human body.

412
00:22:56 --> 00:22:59
It runs all away from the head,
down into the lower intestine.

413
00:22:59 --> 00:23:01
Latin for wanderer.


414
00:23:01 --> 00:23:03
Does amazing things,
like control whether

415
00:23:03 --> 00:23:07
you're hungry or not.


416
00:23:07 --> 00:23:10
Its main, one of its main
functions, is it controls

417
00:23:10 --> 00:23:13
the parasympathetic nervous
system for the heart.

418
00:23:13 --> 00:23:16
And it's the nerve that
the brain uses to tell

419
00:23:16 --> 00:23:18
the heart, slow down.


420
00:23:18 --> 00:23:21
But, in addition to
transmitting down from

421
00:23:21 --> 00:23:24
the brain, you can
transmit up the nerve.

422
00:23:24 --> 00:23:28
And in fact, the left one, for
reasons I don't understand,

423
00:23:28 --> 00:23:31
translates much better up
than the right one does.

424
00:23:31 --> 00:23:34
Which transmits
much better down.

425
00:23:34 --> 00:23:36
So use the left one.


426
00:23:36 --> 00:23:39
And the notion is, if at the
start of a seizure, we put a

427
00:23:39 --> 00:23:42
current on that wire, we
stimulate the brain and

428
00:23:42 --> 00:23:44
we stop the seizure.


429
00:23:44 --> 00:23:46
That's the theory.


430
00:23:46 --> 00:23:50
It's been shown to stop
seizures in rats.

431
00:23:50 --> 00:23:54
But I don't much care
about helping rats.

432
00:23:54 --> 00:23:58
People purport to have
shown it stops seizures

433
00:23:58 --> 00:24:00
in adults, or in people.


434
00:24:00 --> 00:24:04
The level of evidence is,
honestly, ambiguous.

435
00:24:04 --> 00:24:09
Because what they've done is,
they've implanted it, and they

436
00:24:09 --> 00:24:11
give the patient a magnet.


437
00:24:11 --> 00:24:13
And say, when you're having a
seizure, swipe the magnet

438
00:24:13 --> 00:24:17
across, sorry about that,
swipe the magnet across the

439
00:24:17 --> 00:24:21
stimulator, it will turn it
on and stop your seizure.

440
00:24:21 --> 00:24:23
Well, could you imagine that
little girl trying to swipe

441
00:24:23 --> 00:24:26
a magnet across her chest?


442
00:24:26 --> 00:24:27
Not likely.


443
00:24:27 --> 00:24:33
So most people don't
do it successfully.

444
00:24:33 --> 00:24:37
Some people report, I thought I
was about to have a seizure, I

445
00:24:37 --> 00:24:40
swiped the magnet and
I didn't have it.

446
00:24:40 --> 00:24:43
Well, how do we know?


447
00:24:43 --> 00:24:45
Did they really stop it, or
did they just imagine they

448
00:24:45 --> 00:24:47
were going to have one?


449
00:24:47 --> 00:24:50
So it's pretty ambiguous, but
we're now in the process of

450
00:24:50 --> 00:24:54
doing some real tests at Beth
Israel Deaconness

451
00:24:54 --> 00:24:55
Medical Center.


452
00:24:55 --> 00:24:58
And we've been admitting
patients, and controlling

453
00:24:58 --> 00:25:02
the stimulator, and so
far I'm optimistic.

454
00:25:02 --> 00:25:05
All right, one more example.


455
00:25:05 --> 00:25:06
Predicting death.


456
00:25:06 --> 00:25:11
Well, that's kind of
ominous sounding.

457
00:25:11 --> 00:25:14
About one and a quarter million
Americans have an acute

458
00:25:14 --> 00:25:17
coronary syndrome each year.


459
00:25:17 --> 00:25:20
That's to say, some sort
of a cardiac event.

460
00:25:20 --> 00:25:26
A heart attack, or an
arrhythmia of some sort,

461
00:25:26 --> 00:25:29
various kinds of things,
unstable angina.

462
00:25:29 --> 00:25:33
And of that one and a quarter
million people, 15 percent to

463
00:25:33 --> 00:25:36
20 percent of them will die
of a cardiac-related event

464
00:25:36 --> 00:25:39
within the next 4 years.


465
00:25:39 --> 00:25:41
Pretty high.


466
00:25:41 --> 00:25:47
In fact, about 5% within the
next 90 days, there about.

467
00:25:47 --> 00:25:50
So what do you do?


468
00:25:50 --> 00:25:52
The key thing, we
have treatments.

469
00:25:52 --> 00:25:54
We actually know how to
treat most of these things.

470
00:25:54 --> 00:25:57
But we don't know who to
give the treatments to?

471
00:25:57 --> 00:26:00
So, for example, who gets an
implanted defibrillator?

472
00:26:00 --> 00:26:05
Who should be treated
aggressively with statins?

473
00:26:05 --> 00:26:09
We think we've found a new way
to decide who should get which

474
00:26:09 --> 00:26:10
of these kinds of treatments.


475
00:26:10 --> 00:26:13
Something called morphological
variability, which I'll

476
00:26:13 --> 00:26:16
talk about only briefly.


477
00:26:16 --> 00:26:19
We've tested it on a
fairly large database.

478
00:26:19 --> 00:26:23
We have a database with
about 8,000 patients.

479
00:26:23 --> 00:26:27
We've looked at 2 days for each
patient, each day has 24 hours,

480
00:26:27 --> 00:26:30
each hour has 60 minutes, each
minute has, on average,

481
00:26:30 --> 00:26:32
70 heartbeats.


482
00:26:32 --> 00:26:35
That's a lot of heartbeats.


483
00:26:35 --> 00:26:38
Over a billion.


484
00:26:38 --> 00:26:40
So a lot of the techniques
we've talked about this term,

485
00:26:40 --> 00:26:45
about how do you deal with big
things, I live with every day.

486
00:26:45 --> 00:26:48
Actually, more accurately,
my students live with every

487
00:26:48 --> 00:26:51
day, and I commiserate.


488
00:26:51 --> 00:26:56
But the tests are
pretty convincing.

489
00:26:56 --> 00:27:01
As an example, they're
implanted defibrillators, they,

490
00:27:01 --> 00:27:04
too, go under the clavicle.


491
00:27:04 --> 00:27:07
Fortunately we have
two clavicles.

492
00:27:07 --> 00:27:10
And then they connect to the
heart, and they notice, they

493
00:27:10 --> 00:27:14
have a sensor, they notice the
heart is beating very badly,

494
00:27:14 --> 00:27:16
and they shock the heart.


495
00:27:16 --> 00:27:18
So you've all seen medical
shows, where people put paddles

496
00:27:18 --> 00:27:21
on, say, clear, the patient
goes boom boom, and then they

497
00:27:21 --> 00:27:24
say, sinus rhythm restored.


498
00:27:24 --> 00:27:28
Well, this does the same thing,
but you walk around with it.

499
00:27:28 --> 00:27:30
Now, you don't want to turn on
all the time, because as you

500
00:27:30 --> 00:27:35
can see, it delivers
quite a jolt.

501
00:27:35 --> 00:27:38
Interestingly, this is an
article from this fall, from

502
00:27:38 --> 00:27:42
the New York Times, 90 percent
of the people who get one

503
00:27:42 --> 00:27:47
implanted, and the device
itself costs about $50,000, let

504
00:27:47 --> 00:27:50
alone the cost of
implanting it.

505
00:27:50 --> 00:27:55
90 percent receive
negative medical benefit.

506
00:27:55 --> 00:27:58
How do I know that they
received medical benefit?

507
00:27:58 --> 00:28:04
Well, I know that 90% of
them are never turned on.

508
00:28:04 --> 00:28:06
Remember, there's a sensor
that decides whether

509
00:28:06 --> 00:28:07
to apply the shock.


510
00:28:07 --> 00:28:12
90 percent of them, the
sensor never says, turn on.

511
00:28:12 --> 00:28:15
So 90 percent of them can
deliver no benefit because

512
00:28:15 --> 00:28:17
they're never used.


513
00:28:17 --> 00:28:19
On the other hand, clearly
having the surgery to

514
00:28:19 --> 00:28:23
implant this, is some risk.


515
00:28:23 --> 00:28:26
So that's why the benefit is
not only neutral, but negative.

516
00:28:26 --> 00:28:29
You've had fairly serious
surgery, you've risked

517
00:28:29 --> 00:28:34
infection, and, in fact, this
year several people died.

518
00:28:34 --> 00:28:37
I think three or four died
when leads broke off

519
00:28:37 --> 00:28:39
on these things.


520
00:28:39 --> 00:28:43
So, clearly we're putting in
too many of them, if only

521
00:28:43 --> 00:28:45
10 percent are used.


522
00:28:45 --> 00:28:49
On the other hand, roughly
every one minute and 50

523
00:28:49 --> 00:28:52
seconds, somebody in this
country dies a sudden

524
00:28:52 --> 00:28:55
cardiac death.


525
00:28:55 --> 00:28:59
More often once
every two minutes.

526
00:28:59 --> 00:29:05
100 of them during the
course of this lecture.

527
00:29:05 --> 00:29:08
If these people all had
defibrillators implanted,

528
00:29:08 --> 00:29:11
many of them would not die.


529
00:29:11 --> 00:29:16
So, we have a therapy, we just
don't know who to give it to.

530
00:29:16 --> 00:29:18
So, what does this look like?


531
00:29:18 --> 00:29:25
Again, this is pretty
strong stuff.

532
00:29:25 --> 00:29:28
That's what sudden cardiac
death looks like.

533
00:29:28 --> 00:29:34
An apparently healthy
guy, suddenly he's dead.

534
00:29:34 --> 00:29:39
It's things like this that
remind us why we can do real

535
00:29:39 --> 00:29:42
useful things with our
talents or skills.

536
00:29:42 --> 00:29:44
So how do you identify
high risk cases?

537
00:29:44 --> 00:29:47
Lots of different ways, I won't
go through them, but we're

538
00:29:47 --> 00:29:50
focusing again on the
electrical signals

539
00:29:50 --> 00:29:51
in the heart.


540
00:29:51 --> 00:29:54
We're trying to evaluate
the shape of the heartbeat

541
00:29:54 --> 00:29:57
and what's going on.


542
00:29:57 --> 00:29:59
I won't go through details, the
only thing I wanted to point

543
00:29:59 --> 00:30:04
out is, we use dynamic
programming to

544
00:30:04 --> 00:30:05
actually do this.


545
00:30:05 --> 00:30:08
So the key idea in our
method depends upon

546
00:30:08 --> 00:30:13
dynamic programming.


547
00:30:13 --> 00:30:18
We've evaluated it
several times.

548
00:30:18 --> 00:30:20
Here's one evaluation.


549
00:30:20 --> 00:30:24
We took the 25 percent of the
population that we thought

550
00:30:24 --> 00:30:26
was at the highest risk.


551
00:30:26 --> 00:30:30
And you'll notice that within,
actually the first 30 days,

552
00:30:30 --> 00:30:34
4.19 percent of those
people were dead.

553
00:30:34 --> 00:30:36
This is a retrospective
study, by the way.

554
00:30:36 --> 00:30:41
The ECG had been recorded, and
we knew, we had follow-ups on

555
00:30:41 --> 00:30:45
them, and of the people we said
were not, the other 3/4 of

556
00:30:45 --> 00:30:49
the population, a very
small fraction died.

557
00:30:49 --> 00:30:54
So this is called a Kaplan
Meier curve, and it

558
00:30:54 --> 00:30:56
shows a huge gap.


559
00:30:56 --> 00:30:59
And in fact, roughly speaking,
if we think you were at high

560
00:30:59 --> 00:31:02
risk, you were 8 1/2 times more
likely to die within a

561
00:31:02 --> 00:31:06
month, within 90 days.


562
00:31:06 --> 00:31:09
And the good news is, we
don't do this just so we

563
00:31:09 --> 00:31:11
can say, make your will.


564
00:31:11 --> 00:31:13
Get your affairs in order.


565
00:31:13 --> 00:31:15
We actually do this so that we
can say, all right, this is a

566
00:31:15 --> 00:31:19
person who should maybe get a
defibrillator, maybe should

567
00:31:19 --> 00:31:22
have their arteries
cleaned out.

568
00:31:22 --> 00:31:24
Maybe should take
a blood thinner.

569
00:31:24 --> 00:31:28
Once we know who's at risk, we
can do something about it.

570
00:31:28 --> 00:31:33
All right, all I want to do is
just give you a flavor of the

571
00:31:33 --> 00:31:37
kinds of things you've been
learning, are really useful,

572
00:31:37 --> 00:31:42
and can be used to really do
things that are worth doing.

573
00:31:42 --> 00:31:46
So wrapping up the term,
what have you learned?

574
00:31:46 --> 00:31:50
Well, you know, I think
we'll start with this.

575
00:31:50 --> 00:31:53
If you think what you could do
in September, and what you

576
00:31:53 --> 00:31:57
can do now, you've really
come a long way, right?

577
00:31:57 --> 00:32:02
So try and go back and do
problem set number one, and see

578
00:32:02 --> 00:32:06
how long it takes you now, and
how long it took you then.

579
00:32:06 --> 00:32:09
You'll discover that you
really are at a place you

580
00:32:09 --> 00:32:13
were not in September.


581
00:32:13 --> 00:32:16
We covered five major topics.


582
00:32:16 --> 00:32:19
Learning a language for
expressing computations,

583
00:32:19 --> 00:32:20
that's Python.


584
00:32:20 --> 00:32:22
I already talked about
the importance of

585
00:32:22 --> 00:32:25
having a notation.


586
00:32:25 --> 00:32:29
Learning about the process of
writing and debugging programs.

587
00:32:29 --> 00:32:32
But more generally, learning
about the process of moving

588
00:32:32 --> 00:32:36
from some relatively
ambiguous problem statement.

589
00:32:36 --> 00:32:39
As you've all noticed in the
current problem set, we left

590
00:32:39 --> 00:32:42
a lot of things unspecified.


591
00:32:42 --> 00:32:44
You know, and the email was
flying back and forth, and

592
00:32:44 --> 00:32:48
often the answer was,
you figure it out.

593
00:32:48 --> 00:32:51
Because that's one of the
things we want you to learn.

594
00:32:51 --> 00:32:53
To go from some problem
statement to a

595
00:32:53 --> 00:32:57
computational formulation.


596
00:32:57 --> 00:33:01
As a tool to that, you learned
a basic set of recipes, bunch

597
00:33:01 --> 00:33:04
of different algorithms.


598
00:33:04 --> 00:33:07
And you learned how to use
simulations to deal with

599
00:33:07 --> 00:33:12
problems that did not have
nice closed form solutions.

600
00:33:12 --> 00:33:14
Why Python?


601
00:33:14 --> 00:33:18
Well, believe it or not, it's
an easy language to learn,

602
00:33:18 --> 00:33:20
compared to other languages.


603
00:33:20 --> 00:33:23
Simple syntax, it's
interpretive, which helps with

604
00:33:23 --> 00:33:27
the debugging, you don't have
to worry about managing memory

605
00:33:27 --> 00:33:30
as you would in a
language like C.

606
00:33:30 --> 00:33:31
It's modern.


607
00:33:31 --> 00:33:35
It supports, in a fairly
elegant way, object-oriented

608
00:33:35 --> 00:33:39
programming and classes, and
it's increasingly popular.

609
00:33:39 --> 00:33:43
It's used increasing in other
subjects at MIT and other

610
00:33:43 --> 00:33:48
universities, increasing in
industry, and a large and

611
00:33:48 --> 00:33:51
ever-growing set of libraries.


612
00:33:51 --> 00:33:54
We talked about
writing, testing, and

613
00:33:54 --> 00:33:56
debugging programs.


614
00:33:56 --> 00:34:00
And if I had one message I want
you to remember about all

615
00:34:00 --> 00:34:04
of this, it's be slow.


616
00:34:04 --> 00:34:10
Slow down, take it one step at
a time, take your time and be

617
00:34:10 --> 00:34:14
methodical, systematic,
and careful.

618
00:34:14 --> 00:34:17
Understand the problem first.


619
00:34:17 --> 00:34:21
Then think about the overall
structure of the solution, and

620
00:34:21 --> 00:34:25
the algorithms independently of
how you're going to code it up.

621
00:34:25 --> 00:34:27
So you might say, well,
I'm going to use

622
00:34:27 --> 00:34:29
dynamic programming.


623
00:34:29 --> 00:34:32
And that's an independent
decision from Python or

624
00:34:32 --> 00:34:37
Java or C++ , or anything
else to implement it.

625
00:34:37 --> 00:34:40
And in fact, it's pretty easy
to translate a well-organized

626
00:34:40 --> 00:34:44
program from one
language to another.

627
00:34:44 --> 00:34:48
Break it into small parts, and
we talked about unit testing

628
00:34:48 --> 00:34:49
and the importance of that.


629
00:34:49 --> 00:34:54
Nobody can build anything
big in one fell swoop.

630
00:34:54 --> 00:34:58
So, break it into small parts,
identify useful abstractions,

631
00:34:58 --> 00:35:04
both functional and data, code
and unit test each part, and

632
00:35:04 --> 00:35:06
first worry about
functionality.

633
00:35:06 --> 00:35:08
Are you getting the
correct answer?

634
00:35:08 --> 00:35:09
And then efficiency.


635
00:35:09 --> 00:35:12
And a piece of that is,
learn to use pseudo code

636
00:35:12 --> 00:35:15
to help yourself out.


637
00:35:15 --> 00:35:19
As I said earlier, be
systematic when debugging.

638
00:35:19 --> 00:35:22
Think about the
scientific method.

639
00:35:22 --> 00:35:25
It's not just something
that we use to torture

640
00:35:25 --> 00:35:27
middle school students.


641
00:35:27 --> 00:35:30
And when you write a program
and it doesn't work, ask

642
00:35:30 --> 00:35:34
yourself, why it
did what it did?

643
00:35:34 --> 00:35:38
Focus on understanding why the
program is doing what it's

644
00:35:38 --> 00:35:44
doing, rather than why it's not
doing what you wanted it to?

645
00:35:44 --> 00:35:46
Because as soon as you know why
it's doing what it's doing,

646
00:35:46 --> 00:35:48
you'll know how to fix it.


647
00:35:48 --> 00:35:50
But if you skip that
step, you'll waste an

648
00:35:50 --> 00:35:55
awful lot of time.


649
00:35:55 --> 00:35:58
Going from problem
statement to computation.

650
00:35:58 --> 00:36:00
We said a lot of this, you
break the problem into a

651
00:36:00 --> 00:36:03
series of smaller problems.


652
00:36:03 --> 00:36:06
Ideally, reducing it to a
problem you already have

653
00:36:06 --> 00:36:12
code to solve, or somebody
else's code to solve.

654
00:36:12 --> 00:36:16
My first instinct when I have
to solve a problem is, I type,

655
00:36:16 --> 00:36:20
I go to Google, and I type, I
think about, oh this is

656
00:36:20 --> 00:36:23
really an x problem, a
shortest path problem.

657
00:36:23 --> 00:36:27
I type Python shortest
path, and I see what code

658
00:36:27 --> 00:36:30
pops up on my screen.


659
00:36:30 --> 00:36:33
But I can't do that until I've
realized, oh, it's really a

660
00:36:33 --> 00:36:36
shortest path, or it's
really a knapsack.

661
00:36:36 --> 00:36:40
If I type Python select
courses, I won't get

662
00:36:40 --> 00:36:43
anything useful.


663
00:36:43 --> 00:36:46
Think about what kind of
output you'd like to see.

664
00:36:46 --> 00:36:49
I often start thinking about
a program by first asking

665
00:36:49 --> 00:36:52
myself, what's the output
going to look like?

666
00:36:52 --> 00:36:55
And I'll actually write
the output down.

667
00:36:55 --> 00:36:57
And then I'll see if I can't
write a program to generate

668
00:36:57 --> 00:37:01
that kind of output.


669
00:37:01 --> 00:37:04
Often you can take a problem
formulated as an optimization

670
00:37:04 --> 00:37:08
problem, a lot of thing reduce
to finding the minimum or

671
00:37:08 --> 00:37:12
maximum values, satisfying
some set of constraints.

672
00:37:12 --> 00:37:15
So when you're given something
like, how do I find the

673
00:37:15 --> 00:37:20
directions from here to
Providence, think about it as

674
00:37:20 --> 00:37:22
an optimization problem.


675
00:37:22 --> 00:37:26
And also, think about whether
you really have to solve it.

676
00:37:26 --> 00:37:29
It's very often the case that
an approximate solution

677
00:37:29 --> 00:37:31
is all you really need.


678
00:37:31 --> 00:37:34
And they're a lot faster.


679
00:37:34 --> 00:37:38
So, we think about
approximation formally, in the

680
00:37:38 --> 00:37:43
sense of, say, Newton Raphson,
we looked at, successive

681
00:37:43 --> 00:37:44
approximation.


682
00:37:44 --> 00:37:48
But more informally, an
approximation can be,

683
00:37:48 --> 00:37:50
solve a simpler problem.


684
00:37:50 --> 00:37:53
That you don't really need to
solve the problem you thought

685
00:37:53 --> 00:37:56
needed to solve, if you solve
something simpler, it

686
00:37:56 --> 00:37:58
will do just fine.


687
00:37:58 --> 00:38:01
So you don't find the best
solution, you just find

688
00:38:01 --> 00:38:02
a pretty good solution.


689
00:38:02 --> 00:38:07
Or you simplify the assumptions
and say, well, I'm going to

690
00:38:07 --> 00:38:11
find directions, and to make my
problem a little simpler, I'm

691
00:38:11 --> 00:38:17
not going to allow myself to
take any roads except highways

692
00:38:17 --> 00:38:20
to get from city a to city b.


693
00:38:20 --> 00:38:23
It may not be the shortest,
but it's an easier problem

694
00:38:23 --> 00:38:29
to solve because are
so many fewer roads.

695
00:38:29 --> 00:38:31
We talked about algorithms,
we talked about big o

696
00:38:31 --> 00:38:35
notation, orders of growth,
that's very important.

697
00:38:35 --> 00:38:39
We talked a little bit about
amortized analysis, that you

698
00:38:39 --> 00:38:41
might want to, say, sort
something first if you were

699
00:38:41 --> 00:38:43
going to do a lot of searching.


700
00:38:43 --> 00:38:46
We looked at a bunch of
kinds of algorithms.

701
00:38:46 --> 00:38:47
I've listed them here.


702
00:38:47 --> 00:38:49
I'm, by the way, going to send
you a list with all this stuff

703
00:38:49 --> 00:38:52
on it, among other things, so
don't feel you need

704
00:38:52 --> 00:38:54
to transcribe it.


705
00:38:54 --> 00:38:57
But a lot of, surprisingly
large number of algorithms,

706
00:38:57 --> 00:39:00
to cover in one semester,
when you think about it.

707
00:39:00 --> 00:39:03
Exhaustive enumeration,
successive approximation,

708
00:39:03 --> 00:39:06
greedy, divide and
conquer, decision trees,

709
00:39:06 --> 00:39:07
dynamic programming.


710
00:39:07 --> 00:39:09
Quite a lot.


711
00:39:09 --> 00:39:14
Specific algorithms and
specific optimization problems.

712
00:39:14 --> 00:39:18
We spent a lot of
time on simulation.

713
00:39:18 --> 00:39:22
It's important, but maybe not
as important as all that time,

714
00:39:22 --> 00:39:27
relative to the semester, but
it gave me an excuse to talk

715
00:39:27 --> 00:39:30
about some things that
I wanted to cover.

716
00:39:30 --> 00:39:32
So it gave me a framework to
talk a little bit about

717
00:39:32 --> 00:39:35
probability and statistics.


718
00:39:35 --> 00:39:38
And it's my view, personally,
that every MIT student should

719
00:39:38 --> 00:39:40
be made to take such a course.


720
00:39:40 --> 00:39:45
Since they're not, I tried
to sneak it in here.

721
00:39:45 --> 00:39:49
But it's all, it's
important stuff to know.

722
00:39:49 --> 00:39:53
It gave me an excuse to build
some interesting programs.

723
00:39:53 --> 00:39:58
So, what I hope you noticed, is
as I went through these things,

724
00:39:58 --> 00:40:03
I actually tried to develop the
programs incrementally, to talk

725
00:40:03 --> 00:40:07
about the abstractions, to talk
about how I would test them.

726
00:40:07 --> 00:40:12
And so, just as important as
the simulation itself, was

727
00:40:12 --> 00:40:16
the process of building
the simulation.

728
00:40:16 --> 00:40:19
And so I just shows that as a
mechanism to go through some

729
00:40:19 --> 00:40:24
case studies of building
some interesting programs.

730
00:40:24 --> 00:40:27
It let us talk about
random choice.

731
00:40:27 --> 00:40:30
Much of the world is,
or at least appears

732
00:40:30 --> 00:40:32
to be, stochastic.


733
00:40:32 --> 00:40:36
And therefore, we have to
model randomness in programs.

734
00:40:36 --> 00:40:40
And even we don't have to, it
can be used to solve problems

735
00:40:40 --> 00:40:46
that are not inherently random,
like finding the value of pi.

736
00:40:46 --> 00:40:48
It let us look at the
issue of assessing the

737
00:40:48 --> 00:40:50
quality of an answer.


738
00:40:50 --> 00:40:52
We look at things, said,
do we believe that that

739
00:40:52 --> 00:40:54
answer is right or not?


740
00:40:54 --> 00:40:58
And building models of, at
least parts of, the world.

741
00:40:58 --> 00:41:01
Some pervasive themes
that we talked about.

742
00:41:01 --> 00:41:05
The power of abstraction,
systematic problem solving,

743
00:41:05 --> 00:41:07
these will be useful even
if you never again write a

744
00:41:07 --> 00:41:10
program in your whole life.


745
00:41:10 --> 00:41:12
What next?


746
00:41:12 --> 00:41:15
Well, many of you, almost all
of you, have worked very

747
00:41:15 --> 00:41:17
hard this semester.


748
00:41:17 --> 00:41:20
I hope you feel that you've got
a return on your investment.

749
00:41:20 --> 00:41:22
Only you know that.


750
00:41:22 --> 00:41:25
As I said earlier, take a look
at the problem sets, and see

751
00:41:25 --> 00:41:28
whether you think you've
learned anything.

752
00:41:28 --> 00:41:32
I'd like you to remember after
you leave this course, that

753
00:41:32 --> 00:41:36
you've got a skill you can
use to solve problems.

754
00:41:36 --> 00:41:38
And when you sit down, and
there's some issue that you

755
00:41:38 --> 00:41:44
don't know, just knock out a
program to get your answer.

756
00:41:44 --> 00:41:48
I do it all the time,
and it's very useful.

757
00:41:48 --> 00:41:50
If you want to take more
courses, there are

758
00:41:50 --> 00:41:52
others you can take.


759
00:41:52 --> 00:41:56
6.01, the first course
for Course 6 majors,

760
00:41:56 --> 00:41:58
you will find easy.


761
00:41:58 --> 00:42:02
Experience says, people who
take 6.00 first don't

762
00:42:02 --> 00:42:05
understand what all the
fuss is about 6.01.

763
00:42:05 --> 00:42:07
You'll also find it
fun and interesting.

764
00:42:07 --> 00:42:11
6.034, the introduction to
AI, is now using Python.

765
00:42:11 --> 00:42:14
You know everything you
need to know to take that.

766
00:42:14 --> 00:42:18
6.005, which is software
engineering, you know

767
00:42:18 --> 00:42:20
everything you need
to take that.

768
00:42:20 --> 00:42:25
And, for many of you, you can
take 6.006, which is a really

769
00:42:25 --> 00:42:28
interesting kind of
algorithms course.

770
00:42:28 --> 00:42:31
More advanced algorithms,
particularly those you who

771
00:42:31 --> 00:42:34
like math would find
6.006 really fun.

772
00:42:34 --> 00:42:36
So if you want to take
more courses, there

773
00:42:36 --> 00:42:38
are courses to take.


774
00:42:38 --> 00:42:41
Thank you all, good luck with
the rest of the semester.

775
00:42:41 --> 00:42:47



